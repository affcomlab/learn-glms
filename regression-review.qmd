---
title: "Regression Recap"
format: html
---

Before jumping into generalized linear models (GLMs), we will recap important foundations of general linear models, such as simple and multiple regression. The key is that generalized linear models are useful when assumptions of general linear models no longer hold or are unreasonable.

## Data Demonstration

In this data demo, we will first review loading <i>R</i> packages and data, then simple and multiple linear regression.

The data for this chapter is toy data about academic performance after obtaining a Ph.D. including salary information. It is composed of both continuous and categorical variables.

-   Data: <a href="./yearspub.csv" download> yearspub.csv </a>

| Variable    | Description                     |     Values | Measurement |
|-------------|:--------------------------------|-----------:|:-----------:|
| `yrs_since_phd` | Years since obtaining Ph.D.     |    Integer |    Scale    |
| `n_cites`   | Number of Academic Citations    |    Integer |    Scale    |
| `salary`    | Current Salary Figures          |    Integer |    Scale    |
| `n_pubs`    | Number of Academic Publications |    Integer |    Scale    |
| `sex`       | Male or Female                  | Characters |   Nominal   |

### Loading the Packages and Reading in Data

In <i>R</i>, packages must be installed before using them. Once installed, then they can be activated in session with the `library()` command.

```{r, warning = FALSE, message=FALSE}
## Load packages
library(tidyverse)
library(easystats)
library(ggeffects)

## Read in data from file
yearspubs <- read_csv("yearspubs.csv")

```

### Prepare Data / Exploratory Data Analysis

It is always a good first step to plot data prior to fitting any model. Here, we consider the relationship between `years_since_phd` and `salary`. Notice that, for the most part, we can see a linear relationship such that salary tends to increase with the number of additional years after obtaining a Ph.D.

```{r}
## Explore salary-seniority relationship
ggplot(data = yearspubs, mapping = aes(x = years_since_phd, y = salary)) + 
  geom_point()
```

When we move to the multiple regression section, it is advantageous to recode the `sex` variable into a factor (i.e., as a categorical variable). We can make use of the `factor()` function for this purpose. 

```{r}
yearspubs$sex <- factor(yearspubs$sex, c("female","male"))
```

### Simple Regression Example

Now, we will fit a simple linear regression that regresses `salary` on `years_since_phd` using the `lm()` function, and consider the results output.

```{r}

## Fit linear model and print parameters
fit <- lm(
  formula = salary ~ years_since_phd,
  data = yearspubs
)
model_parameters(fit) |> print_md()

```

This simple regression line is defined by the formula:

$$ y_{\hat{i}} = \beta_0 + \beta_1 x $$


#### Parameter Table

From the results output, we can assess each parameter:

-   The `intercept` ($\beta_0$) is the estimated salary for a professor with zero years since PhD: \$45113.71, 95% CI: \[\$40114.89, 50112.53\]. This is significantly different from zero (<i>p</i> \< 0.001).

-   The coefficient for `years_since_phd` ($\beta_1$) is \$1400.94, 95% CI: \[\$783.12, \$2018.77\]. Therefore, for every additional year of since PhD was associated with an increase of \$783.12 to \$2018.77. This zero-order effect (since it is the only predictor variable) is significantly different from zero (<i>p</i> \< 0.001) at \$1400.94.

#### Effect Sizes

By standardizing the data and then refitting the data, we can calculate the effect size of the `years_since_phd`.

```{r}
## Calculate effect sizes by standardizing
standardize_parameters(fit) |> print_md()

```

Using Cohen's <i>D</i> guidelines, d = 0.51 which is a medium sized effect.

#### Model Performance

One purpose of model fitting is to demonstrate accurate <i>predictions</i> -- given a new X, can we reasonably predict a new Y? (This is different to <i>inference</i>, which is how well these models explain the data).

Three popular metrics of model fit performance in service of prediction are adjusted R\^2, AIC, and BIC.

```{r}
## Calculate model performance indices
model_performance(fit) |> print_md()

```

When comparing models, a higher adjusted R\^2 is a better fit, and a lower AIC/BIC is a better fit. Here, we aren't comparing two competing models, but it is easy to obtain these metrics with our packages.

#### Ploting Model Predictions

Lastly, the model predictions are easy to plot with `predict_response`. The grey bands show our uncertainty along the best fit line, and you can incorporate data into the plot as well with `show_data = TRUE`. These plots visualize the parameter estimates we assessed earlier in the parameter tables.

```{r}
## Plot model-based predictions
predict_response(fit, terms = "years_since_phd") |> plot()
predict_response(fit, terms = "years_since_phd") |> plot(show_data = TRUE)
```

### Multiple Regression Example 

Now, we will fit a multiple linear regression that regresses `salary` and `sex` on `years_since_phd` using the `lm()` function, and consider the results/output.

 
```{r}
## Fit linear model and print parameters
fit2 <- lm(
  formula = salary ~ years_since_phd + sex,
  data = yearspubs
)
model_parameters(fit2) |> print_md()

```

This multiple regression is defined by the formula:

$$ y_{\hat{i}} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 $$
Multiple regression, therefore, allows us the ability to control the relationship of multiple predictors on the outcome variable. Statistically "controlling" for a predictors is when the shared variance between predictors is accounted for/removed so that the unique effect of any one predictor variable on the outcome variable is estimated. We then can ask the following questions:

  - What is the relationship between $x_1$ and $y$ if we hold $x2$ at a constant value? e.g., What is the salary difference between prof A (woman, 10 years) and prof B (woman, 11 years)? 
  
  - If we already know $x2$, how much does learning about $x1$ change our prediction of $y$? e.g. how much does learning that prof C is male change our prediction of their salary if we already knew they have 15 years of seniority?

#### Estimating Means

Recall that prior to fitting the multiple regression, we changed `sex` into a categorical variable using the `factor()` function. This is an example of <i>dummy coding</i>. Therefore, once we fit the multiple regression we were able to separate response values based on different factor levels.

In the case of `sex`, we have a binary categorical variable, so we can obtain average values of `salary` at each level of sex while also accounting for (i.e. controlling for) the (partial) effect of `years_since_phd`. 

We will use the `estimate_means()` function to accomplish this. 

```{r}
gmeans <- estimate_means(fit2, by = "sex")
gmeans |> print_md()

```

We can also create a plot to visualize the means. 
```{r}
plot(gmeans)
```

We can now more easily see that, across all `years_since_phd` values, male academics earn on average <i>more</i> salary than women academics. However, as the plot shows, there is variability within each factor level. 

To see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a <i>contrast</i>. 

```{r}
contrasts <- estimate_contrasts(fit2, contrast = "sex")
contrasts |> print_md()
```

Therefore, there is not a statistically significant difference in means across `years_since_phd` between male and female academics, <i>p</i> = 0.130

#### Moderation

Moderation, also known as interactions, asks us to consider the extent to which the effect of one predictor <b> depends on </b> the value on another predictor. It is up to the investigator, guided by theory and previous research, to consider if including interaction terms are relevant for the research question.  

Again, with moderation we include interaction terms like so: 

$$ y_{\hat{i}} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$$
-   $beta_3$ is the slope of the interaction term (the product of two or more predictors)

We can now fit our new model with the interaction term: 

```{r}
## Fit linear model and print parameters
# the * (asterisk) is short form for including the interation term
fit3 <- lm(
  formula = salary ~ years_since_phd * sex,
  data = yearspubs
)
model_parameters(fit3) |> print_md()
```
In this example, the `years_since_phd` by `sex` interaction term was not significant (p = 0.762). 

We can visualize the interaction by making a marginal effects, or "spotlight", plot. 

```{r}
plot(ggpredict(
  model = fit3,
  terms = c("years_since_phd",
            "sex")
))
```
Note that in this multiple regression / moderation model that each level of `sex` has a different slope. We can estimate these slopes, or the "simple" effect of `years_since_phd` on salary for each level of `sex`, with `estimate_slopes()`. The term "simple" is a result of adding the interaction term in the model. 

```{r}
estimate_slopes(fit3, trend = "years_since_phd", by = "sex")
```

  - In this moderation model, the simple slope of the `years_since_phd` was significantly different from zero for men at \$1349.17, but not for women (p = 0.113). 

# Assumptions

The six assumptions of general linear models can be divided into two general areas.

<u>Assumptions about the Formula</u>

-   Correct Functional Form
-   Perfectly Measured Preditors
-   No Collinearity/Multicollinearity

<u> Assumptions about the Residuals</u>

-   Constant Error Variance
-   Independence of Residuals
-   Normality of Residuals

When we utilize GLMs, we are trying to do better than assume that each of these six assumption of general linear models holds. One way to test some of these assumptions visually is to use the `check_model()` function. As an example will apply these checks on the multiple regression model without the interaction term.

```{r}
check_model(fit2)
```

As you can see, this is an easy way to check for assumptions. Given this model, the assumptions seem to fit well. But in the future GLM models, these assumptions will no longer be reasonable. 


# Conclusion

In this recap, we fit simple and multiple regression models with interaction terms. This is great starting point for learning about GLMs. In the future modules, we will take our understanding and interpretations of general linear models into GLMs and learn when GLMs are most appropriate, especially when assumptions do not hold.  
