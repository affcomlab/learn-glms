---
title: "Regression Recap"
format: html
---

Before jumping into generalized linear models (GLMs), we will recap important foundations of general linear models, such as simple and multiple regression. The key is that generalized linear models are useful when assumptions of general linear models no longer hold or are unreasonable.

## Data Demonstration

In this data demo, we will first review loading <i>R</i> packages and data, then simple and multiple linear regression.

The data for this chapter is toy data about academic performance after obtaining a Ph.D. including salary information. It is composed of both continuous and categorical variables.

-   Data: <a href="./yearspub.csv" download> yearspub.csv </a>

| Variable    | Description                     |     Values | Measurement |
|-------------|:--------------------------------|-----------:|:-----------:|
| `yrs_since_phd` | Years since obtaining Ph.D.     |    Integer |    Scale    |
| `n_cites`   | Number of Academic Citations    |    Integer |    Scale    |
| `salary`    | Current Salary Figures          |    Integer |    Scale    |
| `n_pubs`    | Number of Academic Publications |    Integer |    Scale    |
| `sex`       | Male or Female                  | Characters |   Nominal   |

## Loading the Packages and Reading in Data

In <i>R</i>, packages must be installed before using them. Once installed, then they can be activated in session with the `library()` command.

:::{.callout-note}
One way of installing new <i>R</i> packages is to use the `install.packages()` [command](https://www.rdocumentation.org/packages/utils/versions/3.6.2/topics/install.packages)
:::

```{r, warning = FALSE, message=FALSE}
## Load packages
library(tidyverse)
library(easystats)
library(ggeffects)

## Read in data from file
yearspubs <- read_csv("yearspubs.csv")

```
:::{.callout-note}
`read_csv()` comes from the `tidyverse` package. As used here, by default it assumes that the file <i>yearspubs.csv</i> is in the same folder directory as the <i>R</i> analysis file.
:::

:::{.callout-note}
Each package we load provides functionality:

  - `easystats`: For making table outputs from the models. [`easystats` package documentation](https://cran.r-project.org/web/packages/easystats/easystats.pdf)
  
  - `tidyverse`: Provides a suite of easier to use analysis commands. [`tidyverse` package documentation](https://cran.r-project.org/web/packages/tidyverse/tidyverse.pdf)
  
  - `ggeffects`: Provides plotting and visualizations of the models. [`ggeffects` package documentation](https://cran.r-project.org/web/packages/ggeffects/ggeffects.pdf)
  
:::

## Prepare Data / Exploratory Data Analysis

It is always a good first step to plot data prior to fitting any model. 

First, let's consider the distributions, as histograms, of `years_since_phd` and `salary` using the `ggplot()` function[^1]. 

[^1]: ggplot() also from the `tidyverse` package. 

```{r}
ggplot(data=yearspubs, aes(years_since_phd)) + 
  geom_histogram()
```

Notice that `years_since_phd` appears somewhat positively skewed (to the right tail). 

```{r}
ggplot(data=yearspubs, aes(salary)) + 
  geom_histogram()
```

In contrast to `years_since_phd`, notice that `salary` is more normally distributed.

Now, we consider the relationship between `years_since_phd` and `salary` using a scatterplot. Notice that, for the most part, we can see a linear relationship such that salary tends to increase with the number of additional years after obtaining a Ph.D.

```{r}
## Explore salary-seniority relationship
ggplot(data = yearspubs, mapping = aes(x = years_since_phd, y = salary)) + 
  geom_point()
```

# Simple Regression Example

Now, we will fit a simple linear regression with `salary` as the outcome variable and `years_since_phd` as the predictor variable using the `lm()` function, and consider the results. We will then print a summary table of the model results using the `model_parameters()` function from the `easystats` package. Therefore, `model_paramters()` shows the model's estimated coefficient values. 

```{r}
#| collapse: true

## Fit linear model and print parameters
fit <- lm(
  formula = salary ~ years_since_phd,
  data = yearspubs
)
model_parameters(fit)
```

::: {.callout-tip title="lm() syntax"}
When using the `lm()` function, we are telling <i>R</i> to fit a model of the form:

  - outcome = intercept + predictor. 

In this case, `salary` is the outcome variable, and `years_since_phd` is the predictor variable. The intercept is, by default, implicitly calculated in the model. 
:::

By default, this prints in code format as above. However, for the rest of the modules, we will be using a prettier format called <i>markdown</i> format, as such.

```{r}
model_parameters(fit) |> print_md()
```


This simple regression line is defined by the formula:

$$ y_{\hat{i}} = \beta_0 + \beta_1 x $$


## Parameter Table

From the results output, we can assess each parameter:

-   The `intercept` ($\beta_0$) is the estimated salary for a professor with zero years since PhD: \$45113.71, 95% CI: \[\$40114.89, 50112.53\]. This is significantly different from zero (<i>p</i> \< 0.001).

-   The coefficient for `years_since_phd` ($\beta_1$) is \$1400.94, 95% CI: \[\$783.12, \$2018.77\]. Therefore, for every additional year of since PhD was associated with an increase of \$783.12 to \$2018.77. This zero-order effect (since it is the only predictor variable) is significantly different from zero (<i>p</i> \< 0.001) at \$1400.94.

::: {.callout-tip title="95% CI"}
Confidence intervals in the frequentist interpretation is quite wordy but needs to be understood: 

“If we repeated the experiment over and over again (with different samples of the same size) and computed the 95% CI in each sample, then 95% of those intervals would contain the population parameter value.”

Alternative, reasonable interpretations can be as follows: 

“We can be 95% confident that the 95% CI contains the population mean.” 

- This first alternative isn’t our favorite as it implies a view of probability as belief (which isn’t very frequentist). 

“The 95% CI contains highly reasonable estimates of the population mean.”
:::
## Effect Sizes

By standardizing the data and then refitting the data, we can calculate the effect size of the `years_since_phd`.

```{r}
## Calculate effect sizes by standardizing
#standardize_parameters(fit) |> print_md()
model_parameters(fit) |> print_md()

```

Using Cohen's <i>D</i> guidelines, d = 0.51 which is a medium sized effect.

We can also used this standardized model to make the following statement:

  - For every one SD increase in `years_since_phd`, we would expect a 0.51 increase in `salary`, $\beta_1$ = 0.51, 95% CI: [0.28, 0.73], <i>p</i><0.001. 

## Model Performance

One purpose of model fitting is to demonstrate accurate <i>predictions</i> -- given a new X, can we reasonably predict a new Y? (This is different to <i>inference</i>, which is how well these models explain the data).

Three popular metrics of model fit performance in service of prediction are adjusted R\^2, AIC, and BIC.

```{r}
## Calculate model performance indices
model_performance(fit) |> print_md()

```

When comparing models, a higher adjusted R\^2 is a better fit, and a lower AIC/BIC is a better fit. Here, we aren't comparing two competing models, but it is easy to obtain these metrics with our packages.

## Ploting Model Predictions

Lastly, the model predictions are easy to plot with `predict_response`. The grey bands show our uncertainty along the best fit line, and you can incorporate data into the plot as well with `show_data = TRUE`. These plots visualize the parameter estimates we assessed earlier in the parameter tables.

```{r}
## Plot model-based predictions
predict_response(fit, terms = "years_since_phd") |> plot()
predict_response(fit, terms = "years_since_phd") |> plot(show_data = TRUE)
```

# Multiple Regression Examples

Multiple regression allows us to control for the effects of multiple predictors on the outcome. We will divide our recap across different multiple regression scenarios.

## Two Continuous Predictors 

First, we will fit a multiple linear regression with `salary` as the outcome variable and `years_since_phd` and `n_cites` as the predictor variables. 

Let's proceed like before when we introduce a new variable and plot the distribution for `n_pubs`as a histogram.


```{r}
ggplot(data = yearspubs, aes(n_pubs)) +
  geom_histogram()
```

Here's how we can use the `lm()` function to fit a multiple regression with `salary` as the outcome variable and `years_since_phd` and `n_pubs` as the predictor variables.

```{r}
## Fit linear model and print parameters
fit2 <- lm(
  formula = salary ~ years_since_phd + n_pubs,
  data = yearspubs
)
model_parameters(fit2) |> print_md()
```


This multiple regression is defined by the formula:

$$ y_{\hat{i}} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 $$

Multiple regression allows us the ability to control the relationship of multiple predictors on the outcome variable. Statistically "controlling" for a predictors is when the shared variance between predictors is accounted for/removed so that the unique effect of any one predictor variable on the outcome variable is estimated. We then can ask the following questions:

  - What is the relationship between $x_1$ and $y$ if we hold $x2$ at a constant value? e.g., What is the salary difference between prof A (woman, 10 years) and prof B (woman, 11 years)? 
  
  - If we already know $x2$, how much does learning about $x1$ change our prediction of $y$? e.g. how much does learning that prof C is male change our prediction of their salary if we already knew they have 15 years of seniority?
  
### Parameter Table and Interpretation 

From the parameter table above, we can make the following statements: 

  - The intercept is the estimated `salary` for a professor with zero years since PhD and zero publications: $45113.71, 95% CI: [40114.89,50112.53]. 

  - The <i>partial</i> effect of `n_pubs` was not significantly different from zero at \$151.18. When controlling for `years_since_PhD`, each additional publication was associated with an additional −\$95.98 to \$398.34.
  
  - The partial effect of `years_since_phd` was significantly different from zero at \$1075.21. Thus, when controlling for `n_pubs`, each additional year since PhD was associated with an additional \$261.30 to \$1889.11.

::: {.callout-tip title="Partial Effects vs. Zero-Order Effects"}
When we use multiple regression, the $beta$ predictor parameters are called <b>"partial"</b> (i.e., unique) effects since the model is controlling for each predictors' influence on the outcome variable. 

In contrast, with simple linear regression that has only one predictor, that predictor's effect is called a <b>"zero-order"</b> (i.e., total) effect. 
:::
  
## One Dummy Coded Predictor 

In cases of a categorical predictor(s), we need to re-coded them such that each of the categories/groups they represent can be mathematically represented as a code variable. All coding systems turn a categorical variable into g-1 code variables. 

  - So, for example, if we have 6 groups, we need 5 code variables.

The easiest way to turn a categorical predictor into the code variables is to use the `factor()` function. 

```{r}
yearspubs$sex <- factor(yearspubs$sex, levels =  c("female","male"))
```

  - It will use the first level (in this case, "female"), as the reference group.
  - It will name the slope "factorNonref" (e.g., sexMale). 

:::{.callout-note}
When we use functions like `factor()` we have explicitly written out the input arguments, such as <i>levels</i>, in order to show exactly what information the function is processing. 
:::

In order to interpret the results, it's important to understand how the code variables are integrated into the model's formula: 

$$ y_{\hat{i}} = \beta_0 + \beta_1 C_1i $$

  - The intercept $\beta_0$ is the value of $y_{\hat{i}}$ when $C_1$ = 0. In other words, the reference group's average value.
  
  - The slope $\beta_1$ is the change in $y_{\hat{i}}$ when $C_1 + 1$. In other words, it is the group difference in average value, and adding it to the intercept means we are focusing on the non-reference group's outcome value.

Now, we can fit and interpret the model: 

```{r}
fit3 <- lm(formula = salary ~ sex, data = yearspubs)
model_parameters(fit3) |> print_md()
```

Therefore:

  - The average salary of female academics is \$51,502. 
  
  - Males academics make \$6,442 more on average. So their average salary is \$51,502 + \$6,442 = \$57944
  

### Estimating means and plot

We will use the `estimate_means()` function to accomplish the same thing we did above to obtain average values.  

```{r}
gmeans1 <- estimate_means(fit3) 
gmeans1 |> print_md()
```

Lastly, we can visualize like so: 

```{r}
plot(gmeans1)
```

### Contrast

To see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a <i>contrast</i>. 

```{r}
contrasts <- estimate_contrasts(fit3, contrast = "sex")
contrasts |> print_md()
```

Therefore, there is a statistically significant difference between male and female academics, <i>p</i> < 0.05. 

## One Continuous and One Dummy Coded Predictor 

Now, we will fit a multiple linear regression with `salary` as the outcome variable and with `years_since_phd` and `sex` as the predictor variable.

 
```{r}
## Fit linear model and print parameters
fit4 <- lm(
  formula = salary ~ years_since_phd + sex,
  data = yearspubs
)
model_parameters(fit4) |> print_md()

```

### Estimating Means

Recall that prior to fitting the multiple regression, we changed `sex` into a categorical variable using the `factor()` function. This is an example of <i>dummy coding</i>. Therefore, once we fit the multiple regression we were able to separate response values based on different factor levels.

In the case of `sex`, we have a binary categorical variable, so we can obtain average values of `salary` at each level of sex while also accounting for (i.e. controlling for) the (partial) effect of `years_since_phd`. 

We will use the `estimate_means()` function to accomplish this. 

```{r}
gmeans <- estimate_means(fit4, by = "sex")
gmeans |> print_md()

```

We can also create a plot to visualize the means. 
```{r}
plot(gmeans)
```

We can now more easily see that, across all `years_since_phd` values, male academics earn on average <i>more</i> salary than women academics. However, as the plot shows, there is variability within each factor level. 

To see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a <i>contrast</i>. 

```{r}
contrasts <- estimate_contrasts(fit4, contrast = "sex")
contrasts |> print_md()
```

Therefore, there is not a statistically significant difference in means across `years_since_phd` between male and female academics, <i>p</i> = 0.130

## Moderation

Moderation, also known as interactions, asks us to consider the extent to which the effect of one predictor <b> depends on </b> the value on another predictor. It is up to the investigator, guided by theory and previous research, to consider if including interaction terms are relevant for the research question.  

Again, with moderation we include interaction terms like so: 

$$ y_{\hat{i}} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$$
-   $beta_3$ is the slope of the interaction term (the product of two or more predictors)

We can now fit our new model with the interaction term: 

```{r}
## Fit linear model and print parameters
# the * (asterisk) is short form for including the interation term
fit5 <- lm(
  formula = salary ~ years_since_phd * sex,
  data = yearspubs
)
model_parameters(fit5) |> print_md()
```
In this example, the `years_since_phd` by `sex` interaction term was not significant (p = 0.762). 

We can visualize the interaction by making a marginal effects, or "spotlight", plot. 

```{r}
plot(ggpredict(
  model = fit5,
  terms = c("years_since_phd",
            "sex")
))
```
Note that in this multiple regression / moderation model that each level of `sex` has a different slope. We can estimate these slopes, or the "simple" effect of `years_since_phd` on salary for each level of `sex`, with `estimate_slopes()`. The term "simple" is a result of adding the interaction term in the model. 

```{r}
estimate_slopes(fit5, trend = "years_since_phd", by = "sex")
```

  - In this moderation model, the simple slope of the `years_since_phd` was significantly different from zero for men at \$1349.17, but not for women (p = 0.113). 

# Assumptions

The six assumptions of general linear models can be divided into two general areas.

<u>Assumptions about the Formula</u>

-   Correct Functional Form
-   Perfectly Measured Preditors
-   No Collinearity/Multicollinearity

<u> Assumptions about the Residuals</u>

-   Constant Error Variance
-   Independence of Residuals
-   Normality of Residuals

When we utilize GLMs, we are trying to do better than assume that each of these six assumption of general linear models holds. One way to test some of these assumptions visually is to use the `check_model()` function. As an example will apply these checks on the multiple regression model without the interaction term.

```{r}
check_model(fit2)
```

As you can see, this is an easy way to check for assumptions. Given this model, the assumptions seem to fit well. But in the future GLM models, these assumptions will no longer be reasonable. 


# Conclusion

In this recap, we fit simple and multiple regression models with interaction terms. This is great starting point for learning about GLMs. In the future modules, we will take our understanding and interpretations of general linear models into GLMs and learn when GLMs are most appropriate, especially when assumptions do not hold.  
