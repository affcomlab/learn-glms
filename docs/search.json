[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Generalized Linear Modeling",
    "section": "",
    "text": "Welcome\nWelcome to  learn-glms.com . This goal of this website is to create a set of open-source online modules to broaden the range of statistical models taught in graduate psychology programs beyond the standard models of ANOVA and multiple regression. These modules will act as a foundation for students to harness the flexibility of Generalized Linear Models (GLMs) across many common situations in applied research. We contend that the inherent issue that plagues adoption of GLMs in psychology stems from traditional teaching of GLMs which has not stressed how the importance of the shape and nature of the outcome distributions affects the choice of link functions.\n\n\nAbout the Authors\nAaron Matthew Simmons is a PhD student in the Brain, Behavior, and Quantitative Science program at the University of Kansas and a core member of Dr. Jeffrey Girard’s research lab. His research interests focus on Bayesian statistics, latent variable modeling, scale development, and how new modeling frameworks can be leveraged to study a wide range of cognitive and behavioral phenomena.\nDr. Jeffrey Girard is a recognized expert in quantitative methods and directs both the Brain, Behavior, and Quantitative Science doctoral program at the University of Kansas and the Kansas Data Science Consortium (a state-wide collaboration of data science educators funded as part of a large NSF EPSCoR grant). He has extensive experience teaching graduate-level statistics at the University of Kansas and is one of the three co-founders (and primary instructors) of the Statistics, Methods, and Research Training (SMaRT) Workshops limited liability company, which teaches quantitative workshops to hundreds of trainee and professional researchers each year. Most relevant to this project are his expertise in generalized linear (mixed) modeling, frequentist and Bayesian estimation, the R programming language, the Quarto scientific and technical publishing system, the Git version control system, and the GitHub Pages web-hosting platform.\n\n\nFunding\nThese materials were made possible by funding from the APS Fund for Teaching and Public Understanding of Psychological Science. You can read more about the fund [here] (https://www.psychologicalscience.org/members/teaching/fund)."
  },
  {
    "objectID": "intro.html#overview",
    "href": "intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview"
  },
  {
    "objectID": "regression-review.html",
    "href": "regression-review.html",
    "title": "2  RegressionReview",
    "section": "",
    "text": "Before jumping into generalized linear models (GLMs), it is important to recap important foundations of general linear models, such as linear regression. The key is to realize that generalized linear models attempt to perform model fitting when assumptions of linear regression no longer hold or are unreasonable.\n\n1 + 1\n\n[1] 2\n\n\nThe six assumptions of general linear models can be divided into two general areas.\n\n3 Assumptions about the Formula\n\nCorrect Functional Form\nPerfectly Measured Preditors\nNo Collinearity/Multicollinearity\n\n\n\n4 Assumptions about the Residuals\n\nConstant Error Variance\nIndepedence of Residuals\nNormality of Residuals"
  },
  {
    "objectID": "linear-models.html",
    "href": "linear-models.html",
    "title": "3  Linear models",
    "section": "",
    "text": "# Setup\n\n## Load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.2\n✔ bayestestR  0.13.2   ✔ correlation 0.8.5 \n✔ datawizard  0.11.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.1   ✔ modelbased  0.8.8 \n✔ performance 0.12.0   ✔ parameters  0.22.0\n✔ report      0.5.8    ✔ see         0.8.4 \n\nlibrary(ggeffects)\n\n\nAttaching package: 'ggeffects'\n\nThe following object is masked from 'package:easystats':\n\n    install_latest\n\n## Read data from file\npolysim &lt;- read_csv(\"polysim.csv\")\n\nRows: 100 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x, y1, y2, y3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npolysim\n\n# A tibble: 100 × 4\n       x     y1      y2     y3\n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  24.7  0.464   0.660  -3.24\n 2  23.4 -2.87    2.27    7.32\n 3  21.6 -4.27  -11.9     4.69\n 4  24.0  0.137   0.263  11.7 \n 5  20.3 -3.85  -17.7   -51.1 \n 6  21.2 -2.07   -9.68   -2.43\n 7  24.3 -2.75   -0.193   6.20\n 8  26.2  1.21    0.923 -13.1 \n 9  22.6 -2.34   -3.93    7.80\n10  24.8  0.612   3.07   -1.46\n# ℹ 90 more rows\n\n## Plot x-y1 relationship\nggplot(data = polysim, mapping = aes(x = x, y = y1)) + geom_point()\n\n\n\n# ==============================================================================\n\n# LM for a linear relationship example\n\n## Fit linear model\nfit1 &lt;- lm(\n  formula = y1 ~ x,\n  data = polysim\n)\nmodel_parameters(fit1) |&gt; print_md()\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(98)\np\n\n\n\n\n(Intercept)\n-27.89\n0.92\n(-29.72, -26.06)\n-30.25\n&lt; .001\n\n\nx\n1.13\n0.04\n(1.06, 1.20)\n31.13\n&lt; .001\n\n\n\n\npredict_response(fit1, terms = \"x\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1, check = \"linearity\") # good enough\n\n\n\n# ==============================================================================\n\n# LM with raw polynomials example\n\n## Fit raw polynomial model\nfit1b &lt;- lm(\n  formula = y1 ~ x + I(x^2),\n  data = polysim\n)\nmodel_parameters(fit1b) |&gt; print_md() # both are nonsignificant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n-25.97\n9.37\n(-44.57, -7.38)\n-2.77\n0.007\n\n\nx\n0.97\n0.75\n(-0.51, 2.46)\n1.30\n0.197\n\n\nx^2\n3.05e-03\n0.01\n(-0.03, 0.03)\n0.21\n0.838\n\n\n\n\npredict_response(fit1b, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity and collinearity assumptions\ncheck_model(fit1b, check = \"linearity\") # still acceptable\n\n\n\ncheck_collinearity(fit1b) # problematically high\n\n# Check for Multicollinearity\n\nHigh Correlation\n\n   Term    VIF       VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n      x 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n I(x^2) 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n\n# ==============================================================================\n\n# LM with orthogonal polynomials example\n\n## Fit orthogonal polynomial model\nfit1c &lt;- lm(\n  formula = y1 ~ poly(x, degree = 2),\n  data = polysim\n)\nmodel_parameters(fit1c) |&gt; print_md() # first degree is significant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n0.65\n0.10\n(0.45, 0.85)\n6.54\n&lt; .001\n\n\nx (1st degree)\n30.78\n0.99\n(28.80, 32.75)\n30.98\n&lt; .001\n\n\nx (2nd degree)\n0.20\n0.99\n(-1.77, 2.18)\n0.21\n0.838\n\n\n\n\npredict_response(fit1c, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1c, check = \"linearity\") # still acceptable"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  }
]