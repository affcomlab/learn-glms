[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Generalized Linear Modeling",
    "section": "",
    "text": "Welcome\nThis goal of this website is to create a set of open-source online modules to broaden the range of statistical models taught in graduate psychology programs beyond ANOVA and multiple regression. These modules provide a foundation for students to learn about the flexibility of Generalized Linear Models (GLMs) across many common situations in applied research. We address the inherent issue facing the adoption of GLMs in psychology in that traditional teaching of GLMs does not stress the importance of selecting the approriate link functions.\n\n\nAbout the Authors\n Aaron Matthew Simmons  is a PhD student in the Brain, Behavior, and Quantitative Science program at the University of Kansas and a core member of Dr. Jeffrey Girard’s research lab. His research interests focus on Bayesian statistics, latent variable modeling, scale development, and how new modeling frameworks can be leveraged to study a wide range of cognitive and behavioral phenomena.\n Dr. Jeffrey Girard  is a recognized expert in quantitative methods and directs both the Brain, Behavior, and Quantitative Science doctoral program at the University of Kansas and the Kansas Data Science Consortium (a state-wide collaboration of data science educators funded as part of a large NSF EPSCoR grant). He has extensive experience teaching graduate-level statistics at the University of Kansas and is one of the three co-founders (and primary instructors) of the Statistics, Methods, and Research Training (SMaRT) Workshops limited liability company, which teaches quantitative workshops to hundreds of trainee and professional researchers each year. Most relevant to this project are his expertise in generalized linear (mixed) modeling, frequentist and Bayesian estimation, the R programming language, the Quarto scientific and technical publishing system, the Git version control system, and the GitHub Pages web-hosting platform.\n\n\nFunding\nThese materials were made possible by funding from the APS Fund for Teaching and Public Understanding of Psychological Science. You can read more about the fund [here] (https://www.psychologicalscience.org/members/teaching/fund)\n\n\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Overview\nThese materials focus on conceptual foundations of generalized linear modeling (GLMs), specifying them, and interpreting the results. Topics include __________.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regression-review.html#assumptions-about-the-formula",
    "href": "regression-review.html#assumptions-about-the-formula",
    "title": "2  Regression Recap",
    "section": "3.1 Assumptions about the Formula",
    "text": "3.1 Assumptions about the Formula\n\nCorrect Functional Form\nPerfectly Measured Preditors\nNo Collinearity/Multicollinearity"
  },
  {
    "objectID": "regression-review.html#assumptions-about-the-residuals",
    "href": "regression-review.html#assumptions-about-the-residuals",
    "title": "2  Regression Recap",
    "section": "3.2 Assumptions about the Residuals",
    "text": "3.2 Assumptions about the Residuals\n\nConstant Error Variance\nIndepedence of Residuals\nNormality of Residuals"
  },
  {
    "objectID": "linear-models.html",
    "href": "linear-models.html",
    "title": "3  Linear models",
    "section": "",
    "text": "# Setup\n\n## Load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.2\n✔ bayestestR  0.13.2   ✔ correlation 0.8.5 \n✔ datawizard  0.11.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.1   ✔ modelbased  0.8.8 \n✔ performance 0.12.0   ✔ parameters  0.22.0\n✔ report      0.5.8    ✔ see         0.8.4 \n\nlibrary(ggeffects)\n\n\nAttaching package: 'ggeffects'\n\nThe following object is masked from 'package:easystats':\n\n    install_latest\n\n## Read data from file\npolysim &lt;- read_csv(\"polysim.csv\")\n\nRows: 100 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x, y1, y2, y3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npolysim\n\n# A tibble: 100 × 4\n       x     y1      y2     y3\n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  24.7  0.464   0.660  -3.24\n 2  23.4 -2.87    2.27    7.32\n 3  21.6 -4.27  -11.9     4.69\n 4  24.0  0.137   0.263  11.7 \n 5  20.3 -3.85  -17.7   -51.1 \n 6  21.2 -2.07   -9.68   -2.43\n 7  24.3 -2.75   -0.193   6.20\n 8  26.2  1.21    0.923 -13.1 \n 9  22.6 -2.34   -3.93    7.80\n10  24.8  0.612   3.07   -1.46\n# ℹ 90 more rows\n\n## Plot x-y1 relationship\nggplot(data = polysim, mapping = aes(x = x, y = y1)) + geom_point()\n\n\n\n# ==============================================================================\n\n# LM for a linear relationship example\n\n## Fit linear model\nfit1 &lt;- lm(\n  formula = y1 ~ x,\n  data = polysim\n)\nmodel_parameters(fit1) |&gt; print_md()\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(98)\np\n\n\n\n\n(Intercept)\n-27.89\n0.92\n(-29.72, -26.06)\n-30.25\n&lt; .001\n\n\nx\n1.13\n0.04\n(1.06, 1.20)\n31.13\n&lt; .001\n\n\n\n\npredict_response(fit1, terms = \"x\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1, check = \"linearity\") # good enough\n\n\n\n# ==============================================================================\n\n# LM with raw polynomials example\n\n## Fit raw polynomial model\nfit1b &lt;- lm(\n  formula = y1 ~ x + I(x^2),\n  data = polysim\n)\nmodel_parameters(fit1b) |&gt; print_md() # both are nonsignificant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n-25.97\n9.37\n(-44.57, -7.38)\n-2.77\n0.007\n\n\nx\n0.97\n0.75\n(-0.51, 2.46)\n1.30\n0.197\n\n\nx^2\n3.05e-03\n0.01\n(-0.03, 0.03)\n0.21\n0.838\n\n\n\n\npredict_response(fit1b, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity and collinearity assumptions\ncheck_model(fit1b, check = \"linearity\") # still acceptable\n\n\n\ncheck_collinearity(fit1b) # problematically high\n\n# Check for Multicollinearity\n\nHigh Correlation\n\n   Term    VIF       VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n      x 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n I(x^2) 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n\n# ==============================================================================\n\n# LM with orthogonal polynomials example\n\n## Fit orthogonal polynomial model\nfit1c &lt;- lm(\n  formula = y1 ~ poly(x, degree = 2),\n  data = polysim\n)\nmodel_parameters(fit1c) |&gt; print_md() # first degree is significant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n0.65\n0.10\n(0.45, 0.85)\n6.54\n&lt; .001\n\n\nx (1st degree)\n30.78\n0.99\n(28.80, 32.75)\n30.98\n&lt; .001\n\n\nx (2nd degree)\n0.20\n0.99\n(-1.77, 2.18)\n0.21\n0.838\n\n\n\n\npredict_response(fit1c, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1c, check = \"linearity\") # still acceptable"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "You can download the data and worksheets here."
  },
  {
    "objectID": "intro.html#overview",
    "href": "intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nThese materials focus on conceptual foundations of generalized linear modeling (GLMs), specifying them, and interpreting the results. Topics include __________."
  },
  {
    "objectID": "intro.html#goals",
    "href": "intro.html#goals",
    "title": "1  Introduction",
    "section": "1.2 Goals",
    "text": "1.2 Goals",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#prequisites",
    "href": "intro.html#prequisites",
    "title": "1  Introduction",
    "section": "1.3 Prequisites",
    "text": "1.3 Prequisites\nReaders should be comfortable with multiple linear regression, including building regression models, interpreting regression output, and testing for and interpreting regression coefficients including interactions. The first module can be used to test for preparedness. We recommend UCLA’s Statistical Methods and Data Analytics resources and online seminars for a more in-depth review: https://stats.oarc.ucla.edu/other/mult-pkg/seminars/\nIn addition, readers should also be comfortable with foundational concepts in statistics like sampling distributions, Null Hypothesis Significance Testing (NHST), and p-values.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#materials",
    "href": "intro.html#materials",
    "title": "1  Introduction",
    "section": "1.6 Materials",
    "text": "1.6 Materials\nAll materials are available for download in the appendix. The following are available for download:\n\nData: the data used in each chapter\nR Script: an R script of the code used in each chapter\nWorksheet: a worksheet with questions that follows a similar structure to each chapter, but without answers provided\n\nWe recommend that people self-studying download the data and R script and following along with the code and output interpretations in each chapter. Instructors can benefit from downloading the data, code, and worksheets for use in a lab portion in their classes.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regression-review.html#data-demonstration",
    "href": "regression-review.html#data-demonstration",
    "title": "\n2  Regression Recap\n",
    "section": "",
    "text": "Data:  yearspub.csv \n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\nMeasurement\n\n\n\nyrs_since_phd\nYears since obtaining Ph.D.\nInteger\nScale\n\n\nn_cites\nNumber of Academic Citations\nInteger\nScale\n\n\nsalary\nCurrent Salary Figures\nInteger\nScale\n\n\nn_pubs\nNumber of Academic Publications\nInteger\nScale\n\n\nsex\nMale or Female\nCharacters\nNominal\n\n\n\n\n2.1.1 Loading the Packages and Reading in Data\nIn R, packages must be installed before using them. Once installed, then they can be activated in session with the library() command.\n\n## Load packages\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(ggeffects)\n\n## Read in data from file\nyearspubs &lt;- read_csv(\"yearspubs.csv\")\n\n\n2.1.2 Prepare Data / Exploratory Data Analysis\nIt is always a good first step to plot data prior to fitting any model. Here, we consider the relationship between years_since_phd and salary. Notice that, for the most part, we can see a linear relationship such that salary tends to increase with the number of additional years after obtaining a Ph.D.\n\n## Explore salary-seniority relationship\nggplot(data = yearspubs, mapping = aes(x = years_since_phd, y = salary)) + \n  geom_point()\n\n\n\n\n\n\n\nWhen we move to the multiple regression section, it is advantageous to recode the sex variable into a factor (i.e., as a categorical variable). We can make use of the factor() function for this purpose.\n\nyearspubs$sex &lt;- factor(yearspubs$sex, c(\"female\",\"male\"))\n\n\n2.1.3 Simple Regression Example\nNow, we will fit a simple linear regression that regresses salary on years_since_phd using the lm() function, and consider the results output. We will then print a summary table of the model results using the model_parameters() function from the easystats package.\n\n## Fit linear model and print parameters\nfit &lt;- lm(\n  formula = salary ~ years_since_phd,\n  data = yearspubs\n)\nmodel_parameters(fit)\n## Parameter       | Coefficient |      SE |               95% CI | t(60) |      p\n## -------------------------------------------------------------------------------\n## (Intercept)     |    45625.47 | 2473.12 | [40678.49, 50572.46] | 18.45 | &lt; .001\n## years since phd |     1400.94 |  308.87 | [  783.12,  2018.77] |  4.54 | &lt; .001\n## \n## Uncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n##   using a Wald t-distribution approximation.\n\nBy default, this prints in code format as above. However, for the rest of the book, we will be using a prettier format called markdown format, as such.\n\nmodel_parameters(fit) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(60)\np\n\n\n\n(Intercept)\n45625.47\n2473.12\n(40678.49, 50572.46)\n18.45\n&lt; .001\n\n\nyears since phd\n1400.94\n308.87\n(783.12, 2018.77)\n4.54\n&lt; .001\n\n\n\n\n\nThis simple regression line is defined by the formula:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x \\]\n\n2.1.3.1 Parameter Table\nFrom the results output, we can assess each parameter:\n\nThe intercept (\\(\\beta_0\\)) is the estimated salary for a professor with zero years since PhD: $45113.71, 95% CI: [$40114.89, 50112.53]. This is significantly different from zero (p &lt; 0.001).\nThe coefficient for years_since_phd (\\(\\beta_1\\)) is $1400.94, 95% CI: [$783.12, $2018.77]. Therefore, for every additional year of since PhD was associated with an increase of $783.12 to $2018.77. This zero-order effect (since it is the only predictor variable) is significantly different from zero (p &lt; 0.001) at $1400.94.\n\n2.1.3.2 Effect Sizes\nBy standardizing the data and then refitting the data, we can calculate the effect size of the years_since_phd.\n\n## Calculate effect sizes by standardizing\nstandardize_parameters(fit) |&gt; print_md()\n\n\n# Standardization method: refit\n\nParameter\nStd. Coef.\n95% CI\n\n\n\n(Intercept)\n2.11e-16\n[-0.22, 0.22]\n\n\nyears_since_phd\n0.51\n[ 0.28, 0.73]\n\n\n\n\n\nUsing Cohen’s D guidelines, d = 0.51 which is a medium sized effect.\n\n2.1.3.3 Model Performance\nOne purpose of model fitting is to demonstrate accurate predictions – given a new X, can we reasonably predict a new Y? (This is different to inference, which is how well these models explain the data).\nThree popular metrics of model fit performance in service of prediction are adjusted R^2, AIC, and BIC.\n\n## Calculate model performance indices\nmodel_performance(fit) |&gt; print_md()\n\n\nIndices of model performance\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n1325.90\n1326.31\n1332.28\n0.26\n0.24\n10151.69\n10319.50\n\n\n\n\nWhen comparing models, a higher adjusted R^2 is a better fit, and a lower AIC/BIC is a better fit. Here, we aren’t comparing two competing models, but it is easy to obtain these metrics with our packages.\n\n2.1.3.4 Ploting Model Predictions\nLastly, the model predictions are easy to plot with predict_response. The grey bands show our uncertainty along the best fit line, and you can incorporate data into the plot as well with show_data = TRUE. These plots visualize the parameter estimates we assessed earlier in the parameter tables.\n\n## Plot model-based predictions\npredict_response(fit, terms = \"years_since_phd\") |&gt; plot()\n\n\n\n\n\n\npredict_response(fit, terms = \"years_since_phd\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n\n\n\n\n\n2.1.4 Multiple Regression Example\nNow, we will fit a multiple linear regression that regresses salary and sex on years_since_phd using the lm() function, and consider the results/output.\n\n## Fit linear model and print parameters\nfit2 &lt;- lm(\n  formula = salary ~ years_since_phd + sex,\n  data = yearspubs\n)\nmodel_parameters(fit2) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(59)\np\n\n\n\n(Intercept)\n43989.01\n2667.12\n(38652.12, 49325.91)\n16.49\n&lt; .001\n\n\nyears since phd\n1300.30\n312.36\n(675.28, 1925.32)\n4.16\n&lt; .001\n\n\nsex (male)\n4109.49\n2673.09\n(-1239.35, 9458.34)\n1.54\n0.130\n\n\n\n\n\nThis multiple regression is defined by the formula:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] Multiple regression, therefore, allows us the ability to control the relationship of multiple predictors on the outcome variable. Statistically “controlling” for a predictors is when the shared variance between predictors is accounted for/removed so that the unique effect of any one predictor variable on the outcome variable is estimated. We then can ask the following questions:\n\nWhat is the relationship between \\(x_1\\) and \\(y\\) if we hold \\(x2\\) at a constant value? e.g., What is the salary difference between prof A (woman, 10 years) and prof B (woman, 11 years)?\nIf we already know \\(x2\\), how much does learning about \\(x1\\) change our prediction of \\(y\\)? e.g. how much does learning that prof C is male change our prediction of their salary if we already knew they have 15 years of seniority?\n\n\n2.1.4.1 Estimating Means\nRecall that prior to fitting the multiple regression, we changed sex into a categorical variable using the factor() function. This is an example of dummy coding. Therefore, once we fit the multiple regression we were able to separate response values based on different factor levels.\nIn the case of sex, we have a binary categorical variable, so we can obtain average values of salary at each level of sex while also accounting for (i.e. controlling for) the (partial) effect of years_since_phd.\nWe will use the estimate_means() function to accomplish this.\n\ngmeans &lt;- estimate_means(fit2, by = \"sex\")\ngmeans |&gt; print_md()\n\n\nEstimated Marginal Means\n\nsex\nMean\nSE\n95% CI\n\n\n\nfemale\n52818.46\n1989.11\n(48838.27, 56798.66)\n\n\nmale\n56927.96\n1742.00\n(53442.23, 60413.69)\n\n\n\nMarginal means estimated at sex\n\n\nWe can also create a plot to visualize the means.\n\nplot(gmeans)\n\n\n\n\n\n\n\nWe can now more easily see that, across all years_since_phd values, male academics earn on average more salary than women academics. However, as the plot shows, there is variability within each factor level.\nTo see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a contrast.\n\ncontrasts &lt;- estimate_contrasts(fit2, contrast = \"sex\")\ncontrasts |&gt; print_md()\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\nDifference\n95% CI\nSE\nt(59)\np\n\n\nfemale\nmale\n-4109.49\n(-9458.34, 1239.35)\n2673.09\n-1.54\n0.130\n\n\nMarginal contrasts estimated at sex p-value adjustment method: Holm (1979)\n\n\nTherefore, there is not a statistically significant difference in means across years_since_phd between male and female academics, p = 0.130\n\n2.1.5 Moderation\nModeration, also known as interactions, asks us to consider the extent to which the effect of one predictor  depends on  the value on another predictor. It is up to the investigator, guided by theory and previous research, to consider if including interaction terms are relevant for the research question.\nAgain, with moderation we include interaction terms like so:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\] - \\(beta_3\\) is the slope of the interaction term (the product of two or more predictors)\nWe can now fit our new model with the interaction term:\n\n## Fit linear model and print parameters\n# the * (asterisk) is short form for including the interation term\nfit3 &lt;- lm(\n  formula = salary ~ years_since_phd * sex,\n  data = yearspubs\n)\nmodel_parameters(fit3) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(58)\np\n\n\n\n(Intercept)\n45072.28\n4462.64\n(36139.34, 54005.22)\n10.10\n&lt; .001\n\n\nyears since phd\n1112.81\n692.27\n(-272.92, 2498.54)\n1.61\n0.113\n\n\nsex (male)\n2656.21\n5486.15\n(-8325.51, 13637.92)\n0.48\n0.630\n\n\nyears since phd × sex (male)\n236.36\n777.28\n(-1319.53, 1792.25)\n0.30\n0.762\n\n\n\n\n\nIn this example, the years_since_phd by sex interaction term was not significant (p = 0.762).\nWe can visualize the interaction by making a marginal effects, or “spotlight”, plot.\n\nplot(ggpredict(\n  model = fit3,\n  terms = c(\"years_since_phd\",\n            \"sex\")\n))\n\n\n\n\n\n\n\nNote that in this multiple regression / moderation model that each level of sex has a different slope. We can estimate these slopes, or the “simple” effect of years_since_phd on salary for each level of sex, with estimate_slopes(). The term “simple” is a result of adding the interaction term in the model.\n\nestimate_slopes(fit3, trend = \"years_since_phd\", by = \"sex\")\n\nEstimated Marginal Effects\n\nsex    | Coefficient |     SE |             95% CI | t(58) |      p\n-------------------------------------------------------------------\nfemale |     1112.81 | 692.27 | [-272.92, 2498.54] |  1.61 | 0.113 \nmale   |     1349.17 | 353.44 | [ 641.69, 2056.65] |  3.82 | &lt; .001\nMarginal effects estimated for years_since_phd\n\n\n\nIn this moderation model, the simple slope of the years_since_phd was significantly different from zero for men at $1349.17, but not for women (p = 0.113).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression Recap</span>"
    ]
  },
  {
    "objectID": "intro.html#r-packages",
    "href": "intro.html#r-packages",
    "title": "1  Introduction",
    "section": "1.4  R  Packages",
    "text": "1.4  R  Packages\nThese modules will use a variety of  R  packages which can be installed through the  R  CRAN network. We encourage readers to become familiar with the syntax of these packages through documentations pages, which we also supply here. We appreciate these packages because they help to visualization and clarity to model outputs.\n\neasystats: An R framework for easy statistical modeling, visualization, and reporting. easystats package documentation\ntidyverse: A collection of  R  packages designed for data science. tidyverse package documentation\nggeffects: Estimated Marginal Means and Adjusted Predictions from Regression Models. ggeffects package documentation\n\nFor GLM model fitting, modules will utilize the  glm  function in the stats base R package, or the glmmTMB package, for fitting GLMs. We refer readers to the glmmTMB package documentation for further information.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regression-review.html",
    "href": "regression-review.html",
    "title": "\n2  Regression Recap\n",
    "section": "",
    "text": "2.1 Data Demonstration\nIn this data demo, we will first review loading R packages and data, then simple and multiple linear regression.\nThe data for this chapter is toy data about academic performance after obtaining a Ph.D. including salary information. It is composed of both continuous and categorical variables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression Recap</span>"
    ]
  },
  {
    "objectID": "intro.html#textbook-and-website-resources",
    "href": "intro.html#textbook-and-website-resources",
    "title": "1  Introduction",
    "section": "1.5 Textbook and Website Resources",
    "text": "1.5 Textbook and Website Resources\nWe recommend the following textbooks and websites for more in-depth readings of GLMs. We also extend our thanks for these resources as they have helped us create our modules.\n\nFaraway, J.J., (2016).  Extending the linear model with R: Generalized linear, mixed effects and nonparametric regression models  (2nd ed.). CRC Press.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  }
]