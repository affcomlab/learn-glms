[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Generalized Linear Modeling",
    "section": "",
    "text": "Welcome\nThis goal of this website is to create a set of open-source online modules to broaden the range of statistical models taught in graduate psychology programs beyond ANOVA and multiple regression. These modules provide a foundation for students to learn about the flexibility of Generalized Linear Models (GLMs) across many common situations in applied research. We address the inherent issue facing the adoption of GLMs in psychology in that traditional teaching of GLMs does not stress the importance of selecting the approriate link functions.\n\n\nAbout the Authors\n Aaron Matthew Simmons  is a PhD student in the Brain, Behavior, and Quantitative Science program at the University of Kansas and a core member of Dr. Jeffrey Girard’s research lab. His research interests focus on Bayesian statistics, latent variable modeling, scale development, and how new modeling frameworks can be leveraged to study a wide range of cognitive and behavioral phenomena.\n Dr. Jeffrey Girard  is a recognized expert in quantitative methods and directs both the Brain, Behavior, and Quantitative Science doctoral program at the University of Kansas and the Kansas Data Science Consortium (a state-wide collaboration of data science educators funded as part of a large NSF EPSCoR grant). He has extensive experience teaching graduate-level statistics at the University of Kansas and is one of the three co-founders (and primary instructors) of the Statistics, Methods, and Research Training (SMaRT) Workshops limited liability company, which teaches quantitative workshops to hundreds of trainee and professional researchers each year. Most relevant to this project are his expertise in generalized linear (mixed) modeling, frequentist and Bayesian estimation, the R programming language, the Quarto scientific and technical publishing system, the Git version control system, and the GitHub Pages web-hosting platform.\n\n\nFunding\nThese materials were made possible by funding from the APS Fund for Teaching and Public Understanding of Psychological Science. You can read more about the fund [here] (https://www.psychologicalscience.org/members/teaching/fund)\n\n\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Overview\nThese materials focus on conceptual foundations of generalized linear modeling (GLMs), specifying them, and interpreting the results. Topics include __________.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "regression-review.html#assumptions-about-the-formula",
    "href": "regression-review.html#assumptions-about-the-formula",
    "title": "2  Regression Recap",
    "section": "3.1 Assumptions about the Formula",
    "text": "3.1 Assumptions about the Formula\n\nCorrect Functional Form\nPerfectly Measured Preditors\nNo Collinearity/Multicollinearity"
  },
  {
    "objectID": "regression-review.html#assumptions-about-the-residuals",
    "href": "regression-review.html#assumptions-about-the-residuals",
    "title": "2  Regression Recap",
    "section": "3.2 Assumptions about the Residuals",
    "text": "3.2 Assumptions about the Residuals\n\nConstant Error Variance\nIndepedence of Residuals\nNormality of Residuals"
  },
  {
    "objectID": "linear-models.html",
    "href": "linear-models.html",
    "title": "3  Linear models",
    "section": "",
    "text": "# Setup\n\n## Load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.2\n✔ bayestestR  0.13.2   ✔ correlation 0.8.5 \n✔ datawizard  0.11.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.1   ✔ modelbased  0.8.8 \n✔ performance 0.12.0   ✔ parameters  0.22.0\n✔ report      0.5.8    ✔ see         0.8.4 \n\nlibrary(ggeffects)\n\n\nAttaching package: 'ggeffects'\n\nThe following object is masked from 'package:easystats':\n\n    install_latest\n\n## Read data from file\npolysim &lt;- read_csv(\"polysim.csv\")\n\nRows: 100 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x, y1, y2, y3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npolysim\n\n# A tibble: 100 × 4\n       x     y1      y2     y3\n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  24.7  0.464   0.660  -3.24\n 2  23.4 -2.87    2.27    7.32\n 3  21.6 -4.27  -11.9     4.69\n 4  24.0  0.137   0.263  11.7 \n 5  20.3 -3.85  -17.7   -51.1 \n 6  21.2 -2.07   -9.68   -2.43\n 7  24.3 -2.75   -0.193   6.20\n 8  26.2  1.21    0.923 -13.1 \n 9  22.6 -2.34   -3.93    7.80\n10  24.8  0.612   3.07   -1.46\n# ℹ 90 more rows\n\n## Plot x-y1 relationship\nggplot(data = polysim, mapping = aes(x = x, y = y1)) + geom_point()\n\n\n\n# ==============================================================================\n\n# LM for a linear relationship example\n\n## Fit linear model\nfit1 &lt;- lm(\n  formula = y1 ~ x,\n  data = polysim\n)\nmodel_parameters(fit1) |&gt; print_md()\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(98)\np\n\n\n\n\n(Intercept)\n-27.89\n0.92\n(-29.72, -26.06)\n-30.25\n&lt; .001\n\n\nx\n1.13\n0.04\n(1.06, 1.20)\n31.13\n&lt; .001\n\n\n\n\npredict_response(fit1, terms = \"x\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1, check = \"linearity\") # good enough\n\n\n\n# ==============================================================================\n\n# LM with raw polynomials example\n\n## Fit raw polynomial model\nfit1b &lt;- lm(\n  formula = y1 ~ x + I(x^2),\n  data = polysim\n)\nmodel_parameters(fit1b) |&gt; print_md() # both are nonsignificant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n-25.97\n9.37\n(-44.57, -7.38)\n-2.77\n0.007\n\n\nx\n0.97\n0.75\n(-0.51, 2.46)\n1.30\n0.197\n\n\nx^2\n3.05e-03\n0.01\n(-0.03, 0.03)\n0.21\n0.838\n\n\n\n\npredict_response(fit1b, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity and collinearity assumptions\ncheck_model(fit1b, check = \"linearity\") # still acceptable\n\n\n\ncheck_collinearity(fit1b) # problematically high\n\n# Check for Multicollinearity\n\nHigh Correlation\n\n   Term    VIF       VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n      x 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n I(x^2) 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n\n# ==============================================================================\n\n# LM with orthogonal polynomials example\n\n## Fit orthogonal polynomial model\nfit1c &lt;- lm(\n  formula = y1 ~ poly(x, degree = 2),\n  data = polysim\n)\nmodel_parameters(fit1c) |&gt; print_md() # first degree is significant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n0.65\n0.10\n(0.45, 0.85)\n6.54\n&lt; .001\n\n\nx (1st degree)\n30.78\n0.99\n(28.80, 32.75)\n30.98\n&lt; .001\n\n\nx (2nd degree)\n0.20\n0.99\n(-1.77, 2.18)\n0.21\n0.838\n\n\n\n\npredict_response(fit1c, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1c, check = \"linearity\") # still acceptable"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — Appendix",
    "section": "",
    "text": "You can download the data and worksheets here."
  },
  {
    "objectID": "intro.html#overview",
    "href": "intro.html#overview",
    "title": "1  Introduction",
    "section": "1.1 Overview",
    "text": "1.1 Overview\nThese materials focus on conceptual foundations of generalized linear modeling (GLMs), specifying them, and interpreting the results. Topics include __________."
  },
  {
    "objectID": "intro.html#goals",
    "href": "intro.html#goals",
    "title": "1  Introduction",
    "section": "1.2 Goals",
    "text": "1.2 Goals"
  },
  {
    "objectID": "intro.html#prequisites",
    "href": "intro.html#prequisites",
    "title": "1  Introduction",
    "section": "1.3 Prequisites",
    "text": "1.3 Prequisites\nReaders should be comfortable with multiple linear regression, including building regression models, interpreting regression output, and testing for and interpreting regression coefficients including interactions. The first module can be used to test for preparedness. We recommend UCLA’s Statistical Methods and Data Analytics resources and online seminars for a more in-depth review: https://stats.oarc.ucla.edu/other/mult-pkg/seminars/\nIn addition, readers should also be comfortable with foundational concepts in statistics like sampling distributions, Null Hypothesis Significance Testing (NHST), and p-values."
  },
  {
    "objectID": "intro.html#materials",
    "href": "intro.html#materials",
    "title": "1  Introduction",
    "section": "1.8 Materials",
    "text": "1.8 Materials\nAll materials are available for download in the appendix. The following are available for download:\n\nData: the data used in each chapter\nR Script: an R script of the code used in each chapter\nWorksheet: a worksheet with questions that follows a similar structure to each chapter, but without answers provided\n\nWe recommend that people self-studying download the data and R script and following along with the code and output interpretations in each chapter. Instructors can benefit from downloading the data, code, and worksheets for use in a lab portion in their classes."
  },
  {
    "objectID": "regression-review.html#data-demonstration",
    "href": "regression-review.html#data-demonstration",
    "title": "2  Regression Recap",
    "section": "2.1 Data Demonstration",
    "text": "2.1 Data Demonstration\nIn this data demo, we will first review loading R packages and data, and then review key aspects of simple and multiple linear regression.\nThe data for this chapter is toy data about academic performance after obtaining a Ph.D. including salary information. It is composed of both continuous and categorical variables.\n\nData:  yearspub.csv"
  },
  {
    "objectID": "intro.html#r-packages",
    "href": "intro.html#r-packages",
    "title": "1  Introduction",
    "section": "1.4  R  Packages",
    "text": "1.4  R  Packages\nThese modules will use a variety of  R  packages which can be installed through the  R  CRAN network. We encourage readers to become familiar with the syntax of these packages through documentations pages, which we also supply here. We appreciate these packages because they help to visualization and provide clarity to model outputs.\n\neasystats: An R framework for easy statistical modeling, visualization, and reporting. easystats package documentation\ntidyverse: A collection of  R  packages designed for data science. tidyverse package documentation\nggeffects: Estimated Marginal Means and Adjusted Predictions from Regression Models. ggeffects package documentation\n\nFor GLM model fitting, modules will utilize the  glm  function in the stats base R package, or the glmmTMB package, for fitting GLMs. We refer readers to the glmmTMB package documentation for further information."
  },
  {
    "objectID": "regression-review.html",
    "href": "regression-review.html",
    "title": "2  Regression Recap",
    "section": "",
    "text": "3 Simple Regression Example\nNow, we will fit a simple linear regression with salary as the outcome variable and years_since_phd as the predictor variable using the lm() function, and consider the results. We will then print a summary table of the model results using the model_parameters() function from the easystats package. Therefore, model_paramters() shows the model’s estimated coefficient values.\n## Fit linear model and print parameters\nfit &lt;- lm(\n  formula = salary ~ years_since_phd,\n  data = yearspubs\n)\nmodel_parameters(fit)\n## Parameter       | Coefficient |      SE |               95% CI | t(60) |      p\n## -------------------------------------------------------------------------------\n## (Intercept)     |    45625.47 | 2473.12 | [40678.49, 50572.46] | 18.45 | &lt; .001\n## years since phd |     1400.94 |  308.87 | [  783.12,  2018.77] |  4.54 | &lt; .001\n## \n## Uncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n##   using a Wald t-distribution approximation.\nBy default, this prints in code format as above. However, for the rest of the modules, we will be using a prettier format called markdown format with print_md():\nmodel_parameters(fit) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(60)\np\n\n\n\n\n(Intercept)\n45625.47\n2473.12\n(40678.49, 50572.46)\n18.45\n&lt; .001\n\n\nyears since phd\n1400.94\n308.87\n(783.12, 2018.77)\n4.54\n&lt; .001\nThis simple regression line is defined by the formula:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x \\]\nMultiple regression allows us to control for the effects of multiple predictors on the outcome. Statistically “controlling” for a predictors is when the shared variance between predictors is accounted for/removed so that the unique contribution of any one predictor variable on the outcome variable is estimated. We will divide our recap across different multiple regression scenarios.\nThe six assumptions of general linear models can be divided into two general areas. The key is that GLMs are useful when assumptions of general linear models no longer hold or are unreasonable.\nAssumptions about the Formula\nAssumptions about the Residuals\nWhen we utilize GLMs, we are trying to do better than assume that each of these six assumption of general linear models holds. One way to test some of these assumptions visually is to use the check_model() function. As an example will apply these checks on the multiple regression model without the interaction term.\ncheck_model(fit2)\nAs you can see, this is an easy way to check for assumptions. Given this model, the assumptions seem to fit well. But in the future GLM models, these assumptions will no longer be reasonable.\nIn this recap, we fit simple and multiple regression models with interaction terms. This is great starting point for learning about GLMs. In the future modules, we will take our understanding and interpretations of general linear models into GLMs and learn when GLMs are most appropriate, especially when assumptions do not hold."
  },
  {
    "objectID": "intro.html#textbook-and-website-resources",
    "href": "intro.html#textbook-and-website-resources",
    "title": "1  Introduction",
    "section": "1.7 Textbook and Website Resources",
    "text": "1.7 Textbook and Website Resources\nWe recommend the following textbooks and websites for more in-depth readings of GLMs. We also extend our thanks for these resources as they have helped us create our modules.\n\nFaraway, J.J., (2016).  Extending the linear model with R: Generalized linear, mixed effects and nonparametric regression models  (2nd ed.). CRC Press."
  },
  {
    "objectID": "regression-review.html#footnotes",
    "href": "regression-review.html#footnotes",
    "title": "2  Regression Recap",
    "section": "",
    "text": "ggplot() is also included the tidyverse package.↩︎"
  },
  {
    "objectID": "regression-review.html#simple-regression-example",
    "href": "regression-review.html#simple-regression-example",
    "title": "2  Regression Recap",
    "section": "2.2 Simple Regression Example",
    "text": "2.2 Simple Regression Example\nNow, we will fit a simple linear regression that regresses salary on years_since_phd using the lm() function, and consider the results output. We will then print a summary table of the model results using the model_parameters() function from the easystats package.\n\n## Fit linear model and print parameters\nfit &lt;- lm(\n  formula = salary ~ years_since_phd,\n  data = yearspubs\n)\nmodel_parameters(fit)\n## Parameter       | Coefficient |      SE |               95% CI | t(60) |      p\n## -------------------------------------------------------------------------------\n## (Intercept)     |    45625.47 | 2473.12 | [40678.49, 50572.46] | 18.45 | &lt; .001\n## years since phd |     1400.94 |  308.87 | [  783.12,  2018.77] |  4.54 | &lt; .001\n## \n## Uncertainty intervals (equal-tailed) and p-values (two-tailed) computed\n##   using a Wald t-distribution approximation.\n\n\n\n\n\n\n\nlm() syntax\n\n\n\nWhen using the lm() function, we are telling R to fit a model of the form:\n\noutcome = intercept + predictor.\n\nIn this case, salary is the outcome variable, and years_since_phd is the predictor variable. The intercept is, by default, implicitly calculated in the model.\n\n\nBy default, this prints in code format as above. However, for the rest of the book, we will be using a prettier format called markdown format, as such.\n\nmodel_parameters(fit) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(60)\np\n\n\n\n\n(Intercept)\n45625.47\n2473.12\n(40678.49, 50572.46)\n18.45\n&lt; .001\n\n\nyears since phd\n1400.94\n308.87\n(783.12, 2018.77)\n4.54\n&lt; .001\n\n\n\n\n\nThis simple regression line is defined by the formula:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x \\]\n\n2.2.1 Parameter Table\nFrom the results output, we can assess each parameter:\n\nThe intercept (\\(\\beta_0\\)) is the estimated salary for a professor with zero years since PhD: $45113.71, 95% CI: [$40114.89, 50112.53]. This is significantly different from zero (p &lt; 0.001).\nThe coefficient for years_since_phd (\\(\\beta_1\\)) is $1400.94, 95% CI: [$783.12, $2018.77]. Therefore, for every additional year of since PhD was associated with an increase of $783.12 to $2018.77. This zero-order effect (since it is the only predictor variable) is significantly different from zero (p &lt; 0.001) at $1400.94.\n\n\n\n2.2.2 Effect Sizes\nBy standardizing the data and then refitting the data, we can calculate the effect size of the years_since_phd.\n\n## Calculate effect sizes by standardizing\n#standardize_parameters(fit) |&gt; print_md()\nmodel_parameters(fit) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(60)\np\n\n\n\n\n(Intercept)\n45625.47\n2473.12\n(40678.49, 50572.46)\n18.45\n&lt; .001\n\n\nyears since phd\n1400.94\n308.87\n(783.12, 2018.77)\n4.54\n&lt; .001\n\n\n\n\n\nUsing Cohen’s D guidelines, d = 0.51 which is a medium sized effect.\nWe can also used this standardized model to make the following statement:\n\nFor every one SD increase in years_since_phd, we would expect a 0.51 increase in salary, \\(\\beta_1\\) = 0.51, 95% CI: [0.28, 0.73], p&lt;0.001.\n\n\n\n2.2.3 Model Performance\nOne purpose of model fitting is to demonstrate accurate predictions – given a new X, can we reasonably predict a new Y? (This is different to inference, which is how well these models explain the data).\nThree popular metrics of model fit performance in service of prediction are adjusted R^2, AIC, and BIC.\n\n## Calculate model performance indices\nmodel_performance(fit) |&gt; print_md()\n\n\nIndices of model performance\n\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n1325.90\n1326.31\n1332.28\n0.26\n0.24\n10151.69\n10319.50\n\n\n\n\n\nWhen comparing models, a higher adjusted R^2 is a better fit, and a lower AIC/BIC is a better fit. Here, we aren’t comparing two competing models, but it is easy to obtain these metrics with our packages.\n\n\n2.2.4 Ploting Model Predictions\nLastly, the model predictions are easy to plot with predict_response. The grey bands show our uncertainty along the best fit line, and you can incorporate data into the plot as well with show_data = TRUE. These plots visualize the parameter estimates we assessed earlier in the parameter tables.\n\n## Plot model-based predictions\npredict_response(fit, terms = \"years_since_phd\") |&gt; plot()\n\n\n\npredict_response(fit, terms = \"years_since_phd\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting."
  },
  {
    "objectID": "regression-review.html#multiple-regression-example",
    "href": "regression-review.html#multiple-regression-example",
    "title": "2  Regression Recap",
    "section": "2.3 Multiple Regression Example",
    "text": "2.3 Multiple Regression Example\nNow, we will fit a multiple linear regression that regresses salary and sex on years_since_phd using the lm() function, and consider the results/output.\n\n## Fit linear model and print parameters\nfit2 &lt;- lm(\n  formula = salary ~ years_since_phd + sex,\n  data = yearspubs\n)\nmodel_parameters(fit2) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(59)\np\n\n\n\n\n(Intercept)\n43989.01\n2667.12\n(38652.12, 49325.91)\n16.49\n&lt; .001\n\n\nyears since phd\n1300.30\n312.36\n(675.28, 1925.32)\n4.16\n&lt; .001\n\n\nsex (male)\n4109.49\n2673.09\n(-1239.35, 9458.34)\n1.54\n0.130\n\n\n\n\n\nThis multiple regression is defined by the formula:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\] Multiple regression, therefore, allows us the ability to control the relationship of multiple predictors on the outcome variable. Statistically “controlling” for a predictors is when the shared variance between predictors is accounted for/removed so that the unique effect of any one predictor variable on the outcome variable is estimated. We then can ask the following questions:\n\nWhat is the relationship between \\(x_1\\) and \\(y\\) if we hold \\(x2\\) at a constant value? e.g., What is the salary difference between prof A (woman, 10 years) and prof B (woman, 11 years)?\nIf we already know \\(x2\\), how much does learning about \\(x1\\) change our prediction of \\(y\\)? e.g. how much does learning that prof C is male change our prediction of their salary if we already knew they have 15 years of seniority?\n\n\n2.3.1 Estimating Means\nRecall that prior to fitting the multiple regression, we changed sex into a categorical variable using the factor() function. This is an example of dummy coding. Therefore, once we fit the multiple regression we were able to separate response values based on different factor levels.\nIn the case of sex, we have a binary categorical variable, so we can obtain average values of salary at each level of sex while also accounting for (i.e. controlling for) the (partial) effect of years_since_phd.\nWe will use the estimate_means() function to accomplish this.\n\ngmeans &lt;- estimate_means(fit2, by = \"sex\")\ngmeans |&gt; print_md()\n\n\nEstimated Marginal Means\n\n\nsex\nMean\nSE\n95% CI\n\n\n\n\nfemale\n52818.46\n1989.11\n(48838.27, 56798.66)\n\n\nmale\n56927.96\n1742.00\n(53442.23, 60413.69)\n\n\n\nMarginal means estimated at sex\n\n\nWe can also create a plot to visualize the means.\n\nplot(gmeans)\n\n\n\n\nWe can now more easily see that, across all years_since_phd values, male academics earn on average more salary than women academics. However, as the plot shows, there is variability within each factor level.\nTo see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a contrast.\n\ncontrasts &lt;- estimate_contrasts(fit2, contrast = \"sex\")\ncontrasts |&gt; print_md()\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\nDifference\n95% CI\nSE\nt(59)\np\n\n\n\n\nfemale\nmale\n-4109.49\n(-9458.34, 1239.35)\n2673.09\n-1.54\n0.130\n\n\n\nMarginal contrasts estimated at sex p-value adjustment method: Holm (1979)\n\n\nTherefore, there is not a statistically significant difference in means across years_since_phd between male and female academics, p = 0.130\n\n\n2.3.2 Moderation\nModeration, also known as interactions, asks us to consider the extent to which the effect of one predictor  depends on  the value on another predictor. It is up to the investigator, guided by theory and previous research, to consider if including interaction terms are relevant for the research question.\nAgain, with moderation we include interaction terms like so:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\] - \\(beta_3\\) is the slope of the interaction term (the product of two or more predictors)\nWe can now fit our new model with the interaction term:\n\n## Fit linear model and print parameters\n# the * (asterisk) is short form for including the interation term\nfit3 &lt;- lm(\n  formula = salary ~ years_since_phd * sex,\n  data = yearspubs\n)\nmodel_parameters(fit3) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(58)\np\n\n\n\n\n(Intercept)\n45072.28\n4462.64\n(36139.34, 54005.22)\n10.10\n&lt; .001\n\n\nyears since phd\n1112.81\n692.27\n(-272.92, 2498.54)\n1.61\n0.113\n\n\nsex (male)\n2656.21\n5486.15\n(-8325.51, 13637.92)\n0.48\n0.630\n\n\nyears since phd × sex (male)\n236.36\n777.28\n(-1319.53, 1792.25)\n0.30\n0.762\n\n\n\n\n\nIn this example, the years_since_phd by sex interaction term was not significant (p = 0.762).\nWe can visualize the interaction by making a marginal effects, or “spotlight”, plot.\n\nplot(ggpredict(\n  model = fit3,\n  terms = c(\"years_since_phd\",\n            \"sex\")\n))\n\n\n\n\nNote that in this multiple regression / moderation model that each level of sex has a different slope. We can estimate these slopes, or the “simple” effect of years_since_phd on salary for each level of sex, with estimate_slopes(). The term “simple” is a result of adding the interaction term in the model.\n\nestimate_slopes(fit3, trend = \"years_since_phd\", by = \"sex\")\n\nEstimated Marginal Effects\n\nsex    | Coefficient |     SE |             95% CI | t(58) |      p\n-------------------------------------------------------------------\nfemale |     1112.81 | 692.27 | [-272.92, 2498.54] |  1.61 | 0.113 \nmale   |     1349.17 | 353.44 | [ 641.69, 2056.65] |  3.82 | &lt; .001\nMarginal effects estimated for years_since_phd\n\n\n\nIn this moderation model, the simple slope of the years_since_phd was significantly different from zero for men at $1349.17, but not for women (p = 0.113)."
  },
  {
    "objectID": "regression-review.html#parameter-table",
    "href": "regression-review.html#parameter-table",
    "title": "2  Regression Recap",
    "section": "3.1 Parameter Table",
    "text": "3.1 Parameter Table\nFrom the results output, we can assess each parameter:\n\nThe intercept (\\(\\beta_0\\)) is the estimated salary for a professor with zero years since PhD: $45113.71, 95% CI: [$40114.89, 50112.53]. This is significantly different from zero (p &lt; 0.001).\nThe coefficient for years_since_phd (\\(\\beta_1\\)) is $1400.94, 95% CI: [$783.12, $2018.77]. Therefore, for every additional year of since PhD was associated with an increase of $783.12 to $2018.77. This zero-order effect (since it is the only predictor variable) is significantly different from zero (p &lt; 0.001) at $1400.94.\n\n\n\n\n\n\n\n95% CI\n\n\n\nConfidence intervals (CI) in the frequentist interpretation is quite wordy but needs to be understood:\n“If we repeated the experiment over and over again (with different samples of the same size) and computed the 95% CI in each sample, then 95% of those intervals would contain the population parameter value.”\nAlternative, reasonable interpretations can be as follows:\n“We can be 95% confident that the 95% CI contains the population mean.”\n\nThis first alternative isn’t our favorite as it implies a view of probability as belief (which isn’t very frequentist).\n\n“The 95% CI contains highly reasonable estimates of the population mean.”"
  },
  {
    "objectID": "regression-review.html#effect-sizes",
    "href": "regression-review.html#effect-sizes",
    "title": "2  Regression Recap",
    "section": "3.2 Effect Sizes",
    "text": "3.2 Effect Sizes\nBy standardizing the data and then refitting the data, we can calculate the effect size of years_since_phd.\n\n## Calculate effect sizes by standardizing\nstandardize_parameters(fit) |&gt; print_md()\n\n\n# Standardization method: refit\n\n\nParameter\nStd. Coef.\n95% CI\n\n\n\n\n(Intercept)\n2.30e-16\n[-0.22, 0.22]\n\n\nyears_since_phd\n0.51\n[ 0.28, 0.73]\n\n\n\n\n\nUsing Cohen’s D guidelines, d = 0.51 which is a medium sized effect.\nWe can also used this standardized model to make the following statement:\n\nFor every one SD increase in years_since_phd, we would expect a 0.51 increase in salary, \\(\\beta_1\\) = 0.51, 95% CI: [0.28, 0.73], p&lt;0.001."
  },
  {
    "objectID": "regression-review.html#model-performance",
    "href": "regression-review.html#model-performance",
    "title": "2  Regression Recap",
    "section": "3.3 Model Performance",
    "text": "3.3 Model Performance\nOne purpose of model fitting is to demonstrate accurate predictions – given a new X, can we reasonably predict a new Y?\nThree popular metrics of model fit performance in service of prediction are adjusted \\(R^2\\), AIC, and BIC.\n\n## Calculate model performance indices\nmodel_performance(fit) |&gt; print_md()\n\n\nIndices of model performance\n\n\nAIC\nAICc\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n1325.90\n1326.31\n1332.28\n0.26\n0.24\n10151.69\n10319.50\n\n\n\n\n\nWhen comparing models, a higher adjusted \\(R^2\\) is a better fit, and a lower AIC/BIC is a better fit. Here, we aren’t comparing two competing models, but it is easy to obtain these metrics with the packages we loaded."
  },
  {
    "objectID": "regression-review.html#ploting-model-predictions",
    "href": "regression-review.html#ploting-model-predictions",
    "title": "2  Regression Recap",
    "section": "3.4 Ploting Model Predictions",
    "text": "3.4 Ploting Model Predictions\nLastly, the model predictions are easy to plot with predict_response. The grey bands show our uncertainty along the best fit line, and you can incorporate data into the plot as well with show_data = TRUE. These plots visualize the parameter estimates we assessed earlier in the parameter tables.\n\n## Plot model-based predictions\npredict_response(fit, terms = \"years_since_phd\") |&gt; plot()\n\n\n\npredict_response(fit, terms = \"years_since_phd\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting."
  },
  {
    "objectID": "regression-review.html#estimating-means",
    "href": "regression-review.html#estimating-means",
    "title": "2  Regression Recap",
    "section": "4.4 Estimating Means",
    "text": "4.4 Estimating Means\nRecall that prior to fitting the multiple regression, we changed sex into a categorical variable using the factor() function. This is an example of dummy coding. Therefore, once we fit the multiple regression we were able to separate response values based on different factor levels.\nIn the case of sex, we have a binary categorical variable, so we can obtain average values of salary at each level of sex while also accounting for (i.e. controlling for) the (partial) effect of years_since_phd.\nWe will use the estimate_means() function to accomplish this.\n\ngmeans &lt;- estimate_means(fit2, by = \"sex\")\ngmeans |&gt; print_md()\n\n\nEstimated Marginal Means\n\n\nsex\nMean\nSE\n95% CI\n\n\n\n\nfemale\n52818.46\n1989.11\n(48838.27, 56798.66)\n\n\nmale\n56927.96\n1742.00\n(53442.23, 60413.69)\n\n\n\nMarginal means estimated at sex\n\n\nWe can also create a plot to visualize the means.\n\nplot(gmeans)\n\n\n\n\nWe can now more easily see that, across all years_since_phd values, male academics earn on average more salary than women academics. However, as the plot shows, there is variability within each factor level.\nTo see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a contrast.\n\ncontrasts &lt;- estimate_contrasts(fit2, contrast = \"sex\")\ncontrasts |&gt; print_md()\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\nDifference\n95% CI\nSE\nt(59)\np\n\n\n\n\nfemale\nmale\n-4109.49\n(-9458.34, 1239.35)\n2673.09\n-1.54\n0.130\n\n\n\nMarginal contrasts estimated at sex p-value adjustment method: Holm (1979)\n\n\nTherefore, there is not a statistically significant difference in means across years_since_phd between male and female academics, p = 0.130"
  },
  {
    "objectID": "regression-review.html#moderation",
    "href": "regression-review.html#moderation",
    "title": "2  Regression Recap",
    "section": "4.4 Moderation",
    "text": "4.4 Moderation\nModeration, also known as interactions, asks us to consider the extent to which the effect of one predictor  depends on  the value on another predictor. It is up to the investigator, guided by theory and previous research, to consider if including interaction terms are relevant for the research question.\nAgain, with moderation we include interaction terms like so:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\] - \\(beta_3\\) is the slope of the interaction term (the product of two or more predictors)\nWe can now fit our new model with the interaction term:\n\n## Fit linear model and print parameters\n# the * (asterisk) is short form for including the interation term\nfit5 &lt;- lm(\n  formula = salary ~ years_since_phd * sex,\n  data = yearspubs\n)\nmodel_parameters(fit5) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(58)\np\n\n\n\n\n(Intercept)\n45072.28\n4462.64\n(36139.34, 54005.22)\n10.10\n&lt; .001\n\n\nyears since phd\n1112.81\n692.27\n(-272.92, 2498.54)\n1.61\n0.113\n\n\nsex (male)\n2656.21\n5486.15\n(-8325.51, 13637.92)\n0.48\n0.630\n\n\nyears since phd × sex (male)\n236.36\n777.28\n(-1319.53, 1792.25)\n0.30\n0.762\n\n\n\n\n\nIn this example, the years_since_phd by sex interaction term was not significant (p = 0.762).\nWe can visualize the interaction by making a marginal effects, or “spotlight”, plot.\n\nplot(ggpredict(\n  model = fit5,\n  terms = c(\"years_since_phd\",\n            \"sex\")\n)) + mytheme\n\n\n\n\nNote that in this multiple regression / moderation model that each level of sex has a different slope. We can estimate these slopes, or the “simple” effect of years_since_phd on salary for each level of sex, with estimate_slopes(). The term “simple” is a result of adding the interaction term in the model.\n\nestimate_slopes(fit5, trend = \"years_since_phd\", by = \"sex\")\n\nEstimated Marginal Effects\n\nsex    | Coefficient |     SE |             95% CI | t(58) |      p\n-------------------------------------------------------------------\nfemale |     1112.81 | 692.27 | [-272.92, 2498.54] |  1.61 | 0.113 \nmale   |     1349.17 | 353.44 | [ 641.69, 2056.65] |  3.82 | &lt; .001\nMarginal effects estimated for years_since_phd\n\n\n\nIn this moderation model, the simple slope of the years_since_phd was significantly different from zero for men at $1349.17, but not for women (p = 0.113)."
  },
  {
    "objectID": "regression-review.html#prepare-data-exploratory-data-analysis",
    "href": "regression-review.html#prepare-data-exploratory-data-analysis",
    "title": "2  Regression Recap",
    "section": "2.3 Prepare Data / Exploratory Data Analysis",
    "text": "2.3 Prepare Data / Exploratory Data Analysis\nIt is always a good first step to plot data prior to fitting any model.\nFirst, let’s consider the distributions, as histograms, of years_since_phd and salary using the ggplot() function1. You can learn more about how to use ggplot() here.\n\nggplot(data=yearspubs, aes(years_since_phd)) + \n  geom_histogram() + mytheme\n\n\n\n\nNotice that years_since_phd appears somewhat positively skewed (to the right tail).\n\nggplot(data=yearspubs, aes(salary)) + \n  geom_histogram() + mytheme\n\n\n\n\nIn contrast to years_since_phd, notice that salary is more normally distributed.\nNow, we consider the relationship between years_since_phd and salary using a scatter plot. Notice that, for the most part, we can see a linear relationship such that salary tends to increase with the number of additional years after obtaining a Ph.D.\n\n## Explore salary-seniority relationship\nggplot(data = yearspubs, mapping = aes(x = years_since_phd, y = salary)) + \n  geom_point() + mytheme"
  },
  {
    "objectID": "regression-review.html#loading-the-packages-and-reading-in-data",
    "href": "regression-review.html#loading-the-packages-and-reading-in-data",
    "title": "2  Regression Recap",
    "section": "2.2 Loading the Packages and Reading in Data",
    "text": "2.2 Loading the Packages and Reading in Data\nIn R, packages must be installed before using them. Once installed, then they can be activated in session with the library() command.\n\n\n\n\n\n\nNote\n\n\n\nOne way of installing new R packages is to use the install.packages() command\n\n\n\n## Load packages\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(ggeffects)\n\n## Read in data from file\nyearspubs &lt;- read_csv(\"yearspubs.csv\")\nyearspubs #print \n\n# A tibble: 62 × 5\n   years_since_phd n_pubs sex    n_cites salary\n             &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1               3     18 female      50  21876\n 2               6      3 female      26  54511\n 3               3      2 female      50  53425\n 4               8     17 male        34  61863\n 5               9     11 female      41  52926\n 6               6      6 male        37  47034\n 7              16     38 male        48  66432\n 8              10     48 male        56  61100\n 9               2      9 male        19  41934\n10               5     22 male        29  97454\n# ℹ 52 more rows\n\n\n\n\n\n\n\n\nNote\n\n\n\nread_csv() comes from the tidyverse package. As used here, by default it assumes that the file yearspubs.csv is in the same folder directory as the R analysis file.\n\n\n\n\n\n\n\n\nNote\n\n\n\nEach package we load provides functionality:\n\neasystats: For making table outputs from the models. easystats website\ntidyverse: Provides a suite of easier to use analysis commands. tidyverse website\nggeffects: Provides plotting and visualizations of the models. ggeffects website\n\n\n\nHere’s a table of our variables:\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\nMeasurement\n\n\n\n\nyrs_since_phd\nYears since obtaining Ph.D.\nInteger\nScale\n\n\nn_cites\nNumber of Academic Citations\nInteger\nScale\n\n\nsalary\nCurrent Salary Figures\nInteger\nScale\n\n\nn_pubs\nNumber of Academic Publications\nInteger\nScale\n\n\nsex\nMale or Female\nCharacters\nNominal"
  },
  {
    "objectID": "regression-review.html#two-continuous-predictors",
    "href": "regression-review.html#two-continuous-predictors",
    "title": "2  Regression Recap",
    "section": "4.1 Two Continuous Predictors",
    "text": "4.1 Two Continuous Predictors\nFirst, we will fit a multiple linear regression with salary as the outcome variable and years_since_phd and n_cites as the predictor variables.\nLet’s proceed like before when we introduce a new variable and plot the distribution for n_pubsas a histogram.\n\nggplot(data = yearspubs, aes(n_pubs)) +\n  geom_histogram() + mytheme\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nHere’s how we can use the lm() function to fit a multiple regression with salary as the outcome variable and years_since_phd and n_pubs as the predictor variables.\n\n## Fit linear model and print parameters\nfit2 &lt;- lm(\n  formula = salary ~ years_since_phd + n_pubs,\n  data = yearspubs\n)\nmodel_parameters(fit2) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(59)\np\n\n\n\n\n(Intercept)\n45113.71\n2498.17\n(40114.89, 50112.53)\n18.06\n&lt; .001\n\n\nyears since phd\n1075.21\n406.75\n(261.30, 1889.11)\n2.64\n0.010\n\n\nn pubs\n151.18\n123.52\n(-95.98, 398.34)\n1.22\n0.226\n\n\n\n\n\nThis multiple regression is defined by the formula:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\]\nMultiple regression allows us the ability to control the relationship of multiple predictors on the outcome variable. As aforementioned, statistically “controlling” for a predictors is when the shared variance between predictors is accounted for/removed so that the unique effect of any one predictor variable on the outcome variable is estimated. We then can ask the following questions:\n\nWhat is the relationship between \\(x_1\\) and \\(y\\) if we hold \\(x2\\) at a constant value? e.g., What is the salary difference between prof A (woman, 10 years) and prof B (woman, 11 years)?\nIf we already know \\(x2\\), how much does learning about \\(x1\\) change our prediction of \\(y\\)? e.g. how much does learning that prof C is male change our prediction of their salary if we already knew they have 15 years of seniority?\n\nRemember, as partial effects, we are finding the unique effect of a predictor when controlling for other predictors explicitly in the model. Importantly, we can’t say anything when controlling for variables not in the model.\n\n4.1.1 Parameter Table Interpretation\nFrom the parameter table above, we can make the following statements:\n\nThe intercept is the estimated salary for a professor with zero years since PhD and zero publications: $45113.71, 95% CI: [40114.89,50112.53].\nThe partial effect of n_pubs was not significantly different from zero at $151.18. When controlling for years_since_PhD, each additional publication was associated with an additional −$95.98 to $398.34.\nThe partial effect of years_since_phd was significantly different from zero at $1075.21. Thus, when controlling for n_pubs, each additional year since PhD was associated with an additional $261.30 to $1889.11.\n\n\n\n\n\n\n\nPartial Effects vs. Zero-Order Effects\n\n\n\nWhen we use multiple regression, the slope (\\(beta\\)) parameters are called “partial” (i.e., unique) effects since the model is controlling for each predictors’ influence on the outcome variable.\nIn contrast, with simple linear regression that has only one predictor, that predictor’s effect is called a “zero-order” (i.e., total) effect.\nThe decision to focus on partial effects or zero-order effects depends on the specific aims of the research question beacuse they answer different questions."
  },
  {
    "objectID": "regression-review.html#one-dummy-coded-predictor",
    "href": "regression-review.html#one-dummy-coded-predictor",
    "title": "2  Regression Recap",
    "section": "4.2 One Dummy Coded Predictor",
    "text": "4.2 One Dummy Coded Predictor\nIn cases of a categorical predictor(s), we need to re-coded them such that each of the categories/groups they represent can be mathematically represented as a code variable. All coding systems turn a categorical variable into g-1 code variables.\n\nSo, for example, if we have 6 groups, we need 5 code variables.\n\nThe easiest way to turn a categorical predictor into the code variables is to use the factor() function.\n\nyearspubs$sex &lt;- factor(yearspubs$sex, levels =  c(\"female\",\"male\"))\n\n\nIt will use the first level (in this case, “female”), as the reference group.\nIt will name the slope “factorNonref” (e.g., sexMale).\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we use functions like factor() we have explicitly written out the input arguments, such as levels, in order to show exactly what information the function is processing.\n\n\n\n\n\n\n\n\nThe $\n\n\n\nAs a reminder, the “$” symbol extracts one of the columns in the dataframe.\n\n\nIn order to interpret the results, it’s important to understand how the code variables are integrated into the model’s formula:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 C_{1i} \\]\n\nThe intercept \\(\\beta_0\\) is the value of \\(y_{\\hat{i}}\\) when \\(C_1\\) = 0. In other words, the reference group’s average value.\nThe slope \\(\\beta_1\\) is the change in \\(y_{\\hat{i}}\\) when \\(C_1 + 1\\). In other words, it is the group difference in average value, and adding it to the intercept means we are focusing on the non-reference group’s outcome value.\n\nNote that if you have more than two more categories per variable (here, we only have two groups/categories), you will use the exact same approach but have more than one slope.\nNow, we can fit and interpret the model:\n\nfit3 &lt;- lm(formula = salary ~ sex, data = yearspubs)\nmodel_parameters(fit3) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(60)\np\n\n\n\n\n(Intercept)\n51501.85\n2214.97\n(47071.25, 55932.46)\n23.25\n&lt; .001\n\n\nsex (male)\n6441.78\n2948.02\n(544.86, 12338.69)\n2.19\n0.033\n\n\n\n\n\nTherefore:\n\nThe average salary of female academics is $51,502.\nMales academics make $6,442 more on average. So their average salary is $51,502 + $6,442 = $57944\n\n\n4.2.1 Estimating means and plot\nWe will use the estimate_means() function to accomplish the same thing we did above to obtain average values.\n\ngmeans1 &lt;- estimate_means(fit3, by = c(\"sex\")) \ngmeans1 |&gt; print_md()\n\n\nEstimated Marginal Means\n\n\nsex\nMean\nSE\n95% CI\n\n\n\n\nfemale\n51501.85\n2214.97\n(47071.25, 55932.46)\n\n\nmale\n57943.63\n1945.43\n(54052.18, 61835.07)\n\n\n\nMarginal means estimated at sex\n\n\nLastly, we can visualize like so:\n\nplot(gmeans1)\n\n\n\n\n\n\n4.2.2 Contrast\nTo see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a contrast.\n\ncontrasts &lt;- estimate_contrasts(fit3, contrast = \"sex\")\ncontrasts |&gt; print_md()\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\nDifference\n95% CI\nSE\nt(60)\np\n\n\n\n\nfemale\nmale\n-6441.78\n(-12338.69, -544.86)\n2948.02\n-2.19\n0.033\n\n\n\nMarginal contrasts estimated at sex p-value adjustment method: Holm (1979)\n\n\nTherefore, there is a statistically significant difference between male and female academics, p &lt; 0.05."
  },
  {
    "objectID": "regression-review.html#one-continuous-and-one-dummy-coded-predictor",
    "href": "regression-review.html#one-continuous-and-one-dummy-coded-predictor",
    "title": "2  Regression Recap",
    "section": "4.3 One Continuous and One Dummy Coded Predictor",
    "text": "4.3 One Continuous and One Dummy Coded Predictor\nNow, we will fit a multiple linear regression with salary as the outcome variable and with years_since_phd and sex as the predictor variable.\n\n## Fit linear model and print parameters\nfit4 &lt;- lm(\n  formula = salary ~ years_since_phd + sex,\n  data = yearspubs\n)\nmodel_parameters(fit4) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(59)\np\n\n\n\n\n(Intercept)\n43989.01\n2667.12\n(38652.12, 49325.91)\n16.49\n&lt; .001\n\n\nyears since phd\n1300.30\n312.36\n(675.28, 1925.32)\n4.16\n&lt; .001\n\n\nsex (male)\n4109.49\n2673.09\n(-1239.35, 9458.34)\n1.54\n0.130\n\n\n\n\n\n\n4.3.1 Estimating Means\nRecall that prior to fitting the multiple regression, we changed sex into a categorical variable using the factor() function. This is an example of dummy coding. Therefore, once we fit the multiple regression we were able to separate response values based on different factor levels.\nIn the case of sex, we have a binary categorical variable, so we can obtain average values of salary for each sex category while also accounting for (i.e. controlling for) the (partial) effect of years_since_phd.\nWe will use the estimate_means() function to accomplish this.\n\ngmeans &lt;- estimate_means(fit4, by = \"sex\")\ngmeans |&gt; print_md()\n\n\nEstimated Marginal Means\n\n\nsex\nMean\nSE\n95% CI\n\n\n\n\nfemale\n52818.46\n1989.11\n(48838.27, 56798.66)\n\n\nmale\n56927.96\n1742.00\n(53442.23, 60413.69)\n\n\n\nMarginal means estimated at sex\n\n\nWe can also create a plot to visualize the means.\n\nplot(gmeans)\n\n\n\n\nWe can now more easily see that, averaged over all years_since_phd values, male academics earn on average more salary than women academics. However, as the plot shows, there is variability within each factor level.\nTo see if this apparent difference of average salary between male and female academics is statistically significant, we can perform a contrast.\n\ncontrasts &lt;- estimate_contrasts(fit4, contrast = \"sex\")\ncontrasts |&gt; print_md()\n\n\nMarginal Contrasts Analysis\n\n\n\n\n\n\n\n\n\n\n\nLevel1\nLevel2\nDifference\n95% CI\nSE\nt(59)\np\n\n\n\n\nfemale\nmale\n-4109.49\n(-9458.34, 1239.35)\n2673.09\n-1.54\n0.130\n\n\n\nMarginal contrasts estimated at sex p-value adjustment method: Holm (1979)\n\n\nTherefore, there is not a statistically significant difference in means across years_since_phd between male and female academics, p = 0.130"
  },
  {
    "objectID": "intro.html#piping",
    "href": "intro.html#piping",
    "title": "1  Introduction",
    "section": "1.5 Piping",
    "text": "1.5 Piping\nAs of R 4.1.0, a native pipe operator |&gt; has been introduced, and we will use this pipe operator throughout our modules. We appreciate pipes as a way to declutter our code, and we find that they are not that much difficult to follow.\nFor example, if you wanted to use the sum() function, you would input a vector of numbers into the function as so:\n\nsum(4,5)\n\n[1] 9\n\n\nInstead, with pipes, you can declare the vector and pipe it (i.e., feed it) to the same function and get the same result:\n\nc(4,5) |&gt; sum() \n\n[1] 9\n\n\nThis example is kind of contrived, but it is true that pipes allow for passing results/objects to the next function in an elegant way."
  },
  {
    "objectID": "binary-regression.html#data-demonstration",
    "href": "binary-regression.html#data-demonstration",
    "title": "3  Binary Regression",
    "section": "3.3 Data Demonstration",
    "text": "3.3 Data Demonstration\nThe data for this chapter consists of some records of passengers on the Titanic. The question we will ask and answer with binary regression is if the amount of fare contributed in some way to the survival of passengers. Since survived is a binary outcome (yes or no), this relationship is best assessed with binary regression.\n\nData:  titanic.csv"
  },
  {
    "objectID": "binary-regression.html#loading-the-packages-and-reading-in-data",
    "href": "binary-regression.html#loading-the-packages-and-reading-in-data",
    "title": "3  Binary Regression",
    "section": "3.4 Loading the Packages and Reading in Data",
    "text": "3.4 Loading the Packages and Reading in Data\nIn addition to our usual packages (tidyverse, easystats, and ggeffects), we will also need to load DHARMa and qqplotr in order to make our plots for our binary regression models.\n\n## Load packages\nlibrary(tidyverse)\nlibrary(easystats)\nlibrary(ggeffects)\nlibrary(DHARMa)\nlibrary(qqplotr)\n\n## Read in data from file\ntitanic_data &lt;- read_csv(\"titanic.csv\")\ntitanic_data\n\n# A tibble: 887 × 5\n   sex      age class  fare survived\n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;   \n 1 male      22 3rd    7.25 no      \n 2 female    38 1st   71.3  yes     \n 3 female    26 3rd    7.92 yes     \n 4 female    35 1st   53.1  yes     \n 5 male      35 3rd    8.05 no      \n 6 male      27 3rd    8.46 no      \n 7 male      54 1st   51.9  no      \n 8 male       2 3rd   21.1  no      \n 9 female    27 3rd   11.1  yes     \n10 female    14 2nd   30.1  yes     \n# ℹ 877 more rows\n\n\nThis data shows records for 877 passengers on the Titanic.\nHere’s a table of our variables in this dataset:\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nValues\nMeasurement\n\n\n\n\nsex\nMale or Female\nCharacters\nNominal\n\n\nage\nAge of Passenger\nDouble\nScale\n\n\nclass\nCategory of Passenger Accommodation\nCharacters\nNominal\n\n\nfare\nCost of fare\nDouble\nScale\n\n\nsurvived\n“Yes” or “No” if survived\nCharacters\nNominal"
  },
  {
    "objectID": "binary-regression.html#prepare-data-exploratory-data-analysis",
    "href": "binary-regression.html#prepare-data-exploratory-data-analysis",
    "title": "3  Binary Regression",
    "section": "3.5 Prepare Data / Exploratory Data Analysis",
    "text": "3.5 Prepare Data / Exploratory Data Analysis\nIn order to use the survived variable in our analyses, we need to re-code the outcomes as integer numbers. Here, we will use the mutate() function from the dplyr package (which is a package that automatically loaded with tidyverse) to add a new column to our data which re-codes “Yes” = 1 and “No” = 0.\n\n## Prepare the data for analysis\ntitanic2 &lt;- \n  titanic_data |&gt; \n  mutate(\n    survived_b = case_match(\n      survived, \n      \"no\" ~ 0,\n      \"yes\" ~ 1\n    )\n  )\ntitanic2 #print\n\n# A tibble: 887 × 6\n   sex      age class  fare survived survived_b\n   &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 male      22 3rd    7.25 no                0\n 2 female    38 1st   71.3  yes               1\n 3 female    26 3rd    7.92 yes               1\n 4 female    35 1st   53.1  yes               1\n 5 male      35 3rd    8.05 no                0\n 6 male      27 3rd    8.46 no                0\n 7 male      54 1st   51.9  no                0\n 8 male       2 3rd   21.1  no                0\n 9 female    27 3rd   11.1  yes               1\n10 female    14 2nd   30.1  yes               1\n# ℹ 877 more rows\n\n\nRe-coding these strings/characters into discrete numbers is important for the model’s mathematics to work.\n\n\n\n\n\n\nmutate() and data-wrangling\n\n\n\nmutate() is a function from dplyr that adds a column to the dataframe (the object that holds the data). This isn’t the only way to accomplish our goal of re-coding survived, but it is a fairly elegant, easy, and straightforward way to do so. The trick is to remember that mutate() adds a column to the existing dataframe with the same number of rows as the existing dataframe.\nOur choice of data-wrangling functions are open to reprisal because there are many ways in data analysis and programming to do the same thing.\n\n\n\n3.5.1 Plotting a binary outcome\nLike always, it is important to plot the data prior to fitting any model. Here, we consider if there is some sort of relationship between survived_b and fare.\n\nggplot(titanic2, aes(x = fare, y = survived_b)) +\n  geom_point() + mytheme\n\n\n\nggplot(titanic2, aes(x = fare, y = survived)) +\n  geom_boxplot() + mytheme\n\n\n\n\nThe boxplot helps clarify what the scatter plot can’t easily show: there seems to be some effect of fare on survival, such that the higher the fare, the more mass/points are in having survived the titanic. While this is plot is interesting, we will explore this relationship in our regression models."
  },
  {
    "objectID": "binary-regression.html",
    "href": "binary-regression.html",
    "title": "3  Binary Regression",
    "section": "",
    "text": "4 Fitting the General Linear Model (Not Recommended)\nBefore we fit the binary regression, it would be worthwhile to try fitting the less appropriate (or downright wrong) regular general linear model. We’ll use our old friend lm() to assess the relationship betwen the outcome variable survived_b with the predictor variable fare.\n#simple linear regression\nfit1 &lt;- lm(\n  formula = survived_b ~ fare,\n  data = titanic2\n)\n\nmodel_parameters(fit1) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(885)\np\n\n\n\n\n(Intercept)\n0.30\n0.02\n(0.27, 0.34)\n16.15\n&lt; .001\n\n\nfare\n2.51e-03\n3.18e-04\n(1.88e-03, 3.13e-03)\n7.88\n&lt; .001\nAs the results show, we have a significant effect of fare, such that a small amount of fare (0.00251 dollars) increases the chance of survival. However, it is worth nothing that just because regular lm() “worked”, it doesn’t mean that the model is a good model to use.\nThis is evident when we plot the predictions using the predict_response() function.\npredict_response(fit1, terms = \"fare\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\nNotice that although the data points are only either 0 or 1 (binary), the regression line takes intermediate values between 0 and 1, and extreme values above and below 0 and 1! For example, if a passenger paid $100, then the outcome value would be ~0.55, which is halfway between alive and dead! We can’t have passengers somewhat between alive/dead, or more or less alive/dead!\nIf that wasn’t enough to deter one from using the general linear model for binary responses, taking a look at a plot of the residuals, using check_residuals() 1, should be telling.\n## Check residuals\ncheck_residuals(fit1) |&gt; plot() # looks bad\nYikes! As you can see, the dots do not fall along the line. Once this assumption of the uniformity/normality of residuals are not met, this is a clear sign that the model should not be so easily trusted.\nSo now, finally, we fit our first GLM."
  },
  {
    "objectID": "binary-regression.html#why-binary-regression",
    "href": "binary-regression.html#why-binary-regression",
    "title": "3  Binary Regression",
    "section": "3.1 Why Binary Regression?",
    "text": "3.1 Why Binary Regression?\nWhen the outcome is a binary variable, or when there are only two possible outcomes, there are two essential problems with using the general linear model (e.g., the regular lm() function) . The first essential problem is due to the non-normal shape of the residuals. The second essential problem is that the outcome variable is not continuous and normally distributed. Both problems contribute to general linear models being biased and making impossible predictions.\nBinary regression is a general term that encompasses specific models such as logistic regression and probit regression, and is a way to analyze these kinds of two-outcome data and hold these assumptions more true. We will focus on logistic regression in this module.\n\n\nTerms\n\nBinary Regression: A general term for models for when the outcome variable has only two possible outcomes. \n\nLogistic Regression: A specific binary regression model which uses the logistic function as the link function. \n\nProbit Regression: A specific binary regression model which uses the probit function as the link function. \n\nLink Function: A function that transforms the outcome variable into a continuous and unbounded variable. It is a key aspect of generalized linear models.\n\n\n\n\n\n3.1.1 Shape of Residuals\nThe general linear model assumes that residuals, or the differences between the observed and predicted values of data, are normally distributed. If the residuals are not normally distributed, the model will not make valid inferences or predictions.\n\n\n3.1.2 Continuous and Normally Distributed Outcome\nSince the outcome variable is binary, a quick histogram plot will show that it is not continuous (i.e., it is discrete). This is a scourge for the general linear model since it assumes continuous and normally distributed outcomes, and it will predict both intermediate states/values between the two possible outcomes and states/values above and below the two possible outcomes. For example, if 0 = Alive, and 1 = Dead, then the general linear model may predict a value between 0 and 1 (somewhere between alive and dead), or values below 0 and above 1 (so less alive and more dead?).\nTherefore, to model a binary outcome using the machinery of linear modeling, we need to transform the outcome variable to a continuous and normally distributed outcome. We do that with what’s called a link function. In other words, the link function will take the binary outcome variable and transform it to a continuous and normally distributed outcome variable. By using a link function, we move from the general linear model to a generalized linear model. See more about link functions in the next section.\nBefore we fit our first binary regression, let’s take a look at what is actually happening and why the generalized linear model is similar, but different, to the general linear model."
  },
  {
    "objectID": "binary-regression.html#footnotes",
    "href": "binary-regression.html#footnotes",
    "title": "3  Binary Regression",
    "section": "",
    "text": "check_residuals() requires the package qqplotr to be installed.↩︎"
  },
  {
    "objectID": "binary-regression.html#comparing-simple-regression-to-simple-binary-regression",
    "href": "binary-regression.html#comparing-simple-regression-to-simple-binary-regression",
    "title": "3  Binary Regression",
    "section": "3.2 Comparing Simple Regression to Simple Binary Regression",
    "text": "3.2 Comparing Simple Regression to Simple Binary Regression\nIn simple regression, we are predicting the outcome (\\(y_{{i}}\\)) with a linear (additive) combination of an intercept (\\(\\beta_0\\)) and one predictor variable (\\(x\\)).\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x \\] In simple binary regression, we are still predicting an outcome with a linear (additive) combination of intercept and one predictor variable. We use the term “simple” only because we have one predictor variable in this case.\n\\[ \\eta_{i} = \\beta_0 + \\beta_1 x \\]\nNote that the linearly additive feature that both simple regression and simple binary regression share is what makes these models related and interpretable.\nThe main difference is why, in simple binary regression, did the outcome variable change from \\(y_{{i}}\\) to \\(\\eta_{i}\\)? Because in binary regression, we are no longer predicting the binary outcome event (e.g., “yes/no”), but the  probability  of the binary event occurring. This means that \\(\\eta_{i}\\) is a probability that is bounded between 0 (“no” event) and 1 (“yes” event).\nThe next section will relate \\(y_{\\hat{i}}\\) to \\(\\eta_{i}\\).\n\n3.2.1 Link function: logit\nWe need to make a connection between the probability of an event (\\(\\eta_{i}\\)) to an event outcome (\\(y_{{i}}\\)). This is possible in this way:\n\nThe outcome \\(y_{{i}}\\) for i = 1,…,\\(n\\) events takes values zero or one with the probability of a 1 = \\(p_i\\)\nIn other words, \\(P(Y_i = 1)\\) = \\(p_i\\), or P is the probabilty of a 1 (e.g., “yes” happening).\n\nThat is all just setup to define \\(\\eta_{i}\\).\n\n\\(\\eta_{i}= g(p_i)\\)\n\nThe variable \\(g\\) is our “link function”. It is a function that transforms a probability (\\(p_i\\)) into our \\(\\eta_{i}\\), which is the outcome we are trying to find in binary regression (i.e., the probabilty of an event occuring.)\nIn binary regression, our \\(g\\) is the “logit function”, which is defined as:\n\\[ \\eta = log(p/(1-p)) \\] or, equivalently,\n\\[ p = e^\\eta/ (1 + e^\\eta) \\]\nWhen we use the logit link function with a linear predictor, we now can call this “logistic regression”.\n\n\n\n\n\n\nTakeaway: Link Functions\n\n\n\nWhen we use GLMs, we will be using many different link functions depending on the data. In the case of binary/logistic regression, all this function is trying to do is relate our linear predictors to a response outcome that is continuous and between some bounds (like 0 and 1). This is because if we did not do this, we are using the linear predictors to predictor some value that is continuous when it is not technically continuous and out of bounds (so, when an event “yes” = 1, then the model will spit out a value like 1.53, which is not possible). We also find that when using the appropriate link function, other problems, like the shape of residuals, also gets fixed!"
  },
  {
    "objectID": "binary-regression.html#fitting-our-binary-regression",
    "href": "binary-regression.html#fitting-our-binary-regression",
    "title": "3  Binary Regression",
    "section": "5.1 Fitting our Binary Regression",
    "text": "5.1 Fitting our Binary Regression\nWhen we fit our binary regression, we now will use the glm() funciton which is similar to lm(), but has one extra input argument. Notice the “family” input argument, which requires which “family of distributions” to use and which “link function” choice.\nFor binary regression, we supply the binomial distribution and the logit link function.\n\nfit2 &lt;- glm(\n  formula = survived_b ~ fare, \n  family = binomial(link = \"logit\"), \n  data = titanic2\n)\n\n## Print parameters in logit (i.e., log-odds) units\nmodel_parameters(fit2) |&gt; print_md()\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-0.93\n0.10\n(-1.12, -0.75)\n-9.79\n&lt; .001\n\n\nfare\n0.02\n2.23e-03\n(0.01, 0.02)\n6.77\n&lt; .001\n\n\n\n\n\n\n5.1.1 Parameter Table Interpretation\nAs seen above, the parameter table output from a model fit with glm() looks very similar to parameter table outputs from models fit with lm(). But there is a big difference – in glm() models, we are no longer working in the same raw units!\nThis difference is apparent in the second column of the output table: “Log-Odds”. Because we fit a logistic regression with a logit function, the parameter values are in “log-odds” unit; it is no longer possible to consider fare in terms of dollar amounts. Technically, when fare is in log-odds units, we can only say:\n\nFor every one 1 unit increase in log-odds of fare is a 0.02 increase in the probability of surviving.\n\nAlthough this is technically true, it isn’t at first clear what “1 unit increase in log-odds of fare” is. But before we completely abandon what log-odds unit can tells and tranform back into our original units (e.g., dollars), let’s see more what using logistic regression can show us.\n\n\n5.1.2 Predicted Probability\nA very useful part of about working with logistic regression is that our outcome variable is also in a transformed space (thanks to the logit function). Our outcome is now “the probability of survival_b”. We can see how this plays out when we plot the predicted probabilities using predict_response(). This function is especially helpful because it keeps the outcome variable in the transformed, continuous space, but transforms the predictors back to their raw units.\n\npredict_response(fit2, terms = \"fare\") |&gt; plot(show_data = TRUE)\n\n\n\n\nAgain, the transformed outcome variable is the “probability of survived_b” (on the y-axis), so we can see that it is bounded between [0,100]. The logistic regression line also makes predictions between these bounds so we no longer have the model predicting unreasonable values/states. Lastly, we can now see how an increase in fare increases the probability of survival.\n\n\n\n\n\n\nCritical Thinking\n\n\n\nWhile the plot above looks very convincing that paying a fare over $400 seems to ensure 100% probability of survival, this model only has one predictor variable. There are a lot of other variables that are not accounted for. Certainly, we would need to consider other variables that may impact survival rates other than fare, and, at the very least, it would be quite surprising if only ’fare` ensured survival."
  },
  {
    "objectID": "intro.html#plot-theme",
    "href": "intro.html#plot-theme",
    "title": "1  Introduction",
    "section": "1.6 Plot theme",
    "text": "1.6 Plot theme\nFor our visualizations, we have set our plots with a template to show a specific layout for aesthetic purposes. We embedded this template in a variable called mytheme and it is in the code with every plot. The settings of this layout is hidden from view for every module, but we display the custome settings here.\n\nlibrary(tidyverse)\nmytheme &lt;- theme_bw(\n  base_size = 12\n)"
  },
  {
    "objectID": "regression-review.html#interactions",
    "href": "regression-review.html#interactions",
    "title": "2  Regression Recap",
    "section": "4.4 Interactions",
    "text": "4.4 Interactions\nInteractions asks us to consider the extent to which the effect of one predictor  depends on  the value on another predictor. It is up to the investigator, guided by theory and previous research, to consider if including interaction terms are relevant for the research question.\nAgain, we include interaction terms like so:\n\\[ y_{\\hat{i}} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2\\] - \\(\\beta_3\\) is the slope of the interaction term (the product of two or more predictors)\nWe can now fit our new model with the interaction term:\n\n## Fit linear model and print parameters\nfit5 &lt;- lm(\n  formula = salary ~ years_since_phd + sex + years_since_phd:sex,\n  data = yearspubs\n)\nmodel_parameters(fit5) |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(58)\np\n\n\n\n\n(Intercept)\n45072.28\n4462.64\n(36139.34, 54005.22)\n10.10\n&lt; .001\n\n\nyears since phd\n1112.81\n692.27\n(-272.92, 2498.54)\n1.61\n0.113\n\n\nsex (male)\n2656.21\n5486.15\n(-8325.51, 13637.92)\n0.48\n0.630\n\n\nyears since phd × sex (male)\n236.36\n777.28\n(-1319.53, 1792.25)\n0.30\n0.762\n\n\n\n\n\nIn this example, the years_since_phd by sex interaction term was not significant (p = 0.762).\n\n\n\n\n\n\nThe * symbol for interactions\n\n\n\nIn the code block above, we specified the interaction term by writing out its explicit form. There is a short-form way in R for writing out all the interaction terms by using the asterisk(*) symbol. This is especially helpful when there are more than two predictor variables where higher-order interaction terms are possible (you would not want to accidentally forget an interaction term!).\nUsing the example above:\n\nLong Form Fomula: salary ~ years_since_phd + sex + years_since_phd:sex\nShort Form Formula:  salary ~ years_since_phd*sex\n\nRemember, both forms are equivalent.\n\n\nWe can visualize the interaction by making a marginal effects, or “spotlight”, plot.\n\nplot(ggpredict(\n  model = fit5,\n  terms = c(\"years_since_phd\",\n            \"sex\")\n)) + mytheme\n\n\n\n\nNote that in this multiple regression with an interaction term that each level of sex has a different slope. We can estimate these slopes, or the “simple” effect of years_since_phd on salary for each category of sex, with estimate_slopes(). The term “simple” is a result of adding the interaction term in the model.\n\nestimate_slopes(fit5, trend = \"years_since_phd\", by = \"sex\") |&gt; print_md()\n\n\nEstimated Marginal Effects\n\n\nsex\nCoefficient\nSE\n95% CI\nt(58)\np\n\n\n\n\nfemale\n1112.81\n692.27\n(-272.92, 2498.54)\n1.61\n0.113\n\n\nmale\n1349.17\n353.44\n( 641.69, 2056.65)\n3.82\n&lt; .001\n\n\n\nMarginal effects estimated for years_since_phd\n\n\n\nIn this model with an interaction, the simple slope of the years_since_phd was significantly different from zero for men at $1349.17, but not for women (p = 0.113). Therefore, it is possible to have a non-significant interaction term, and some, but not all, significant simple slopes."
  }
]