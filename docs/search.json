[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Generalized Linear Modeling",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2\n\n\nThis goal of this  R bookdown  is to create a set of open-source online modules to broaden the range of statistical models taught in graduate psychology programs beyond the standard models of ANOVA and multiple regression. These modules will act as a foundation for students to harness the flexibility of Generalized Linear Models (GLMs) across many common situations in applied research. We contend that the inherent issue that plagues adoption of GLMs in psychology stems from traditional teaching of GLMs which has not stressed how the importance of the shape and nature of the outcome distributions affects the choice of link functions."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\nTEST TEST TEST\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "regression-review.html#assumptions-about-the-formula",
    "href": "regression-review.html#assumptions-about-the-formula",
    "title": "2  RegressionReview",
    "section": "2.1 Assumptions about the Formula",
    "text": "2.1 Assumptions about the Formula\n\nCorrect Functional Form\nPerfectly Measured Preditors\nNo Collinearity/Multicollinearity"
  },
  {
    "objectID": "regression-review.html#assumptions-about-the-residuals",
    "href": "regression-review.html#assumptions-about-the-residuals",
    "title": "2  RegressionReview",
    "section": "2.2 Assumptions about the Residuals",
    "text": "2.2 Assumptions about the Residuals\n\nConstant Error Variance\nIndepedence of Residuals\nNormality of Residuals"
  },
  {
    "objectID": "linear-models.html",
    "href": "linear-models.html",
    "title": "3  Linear models",
    "section": "",
    "text": "# Setup\n\n## Load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(easystats)\n\n# Attaching packages: easystats 0.7.2\n✔ bayestestR  0.13.2   ✔ correlation 0.8.5 \n✔ datawizard  0.11.0   ✔ effectsize  0.8.9 \n✔ insight     0.20.1   ✔ modelbased  0.8.8 \n✔ performance 0.12.0   ✔ parameters  0.22.0\n✔ report      0.5.8    ✔ see         0.8.4 \n\nlibrary(ggeffects)\n\n\nAttaching package: 'ggeffects'\n\nThe following object is masked from 'package:easystats':\n\n    install_latest\n\n## Read data from file\npolysim &lt;- read_csv(\"polysim.csv\")\n\nRows: 100 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (4): x, y1, y2, y3\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npolysim\n\n# A tibble: 100 × 4\n       x     y1      y2     y3\n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1  24.7  0.464   0.660  -3.24\n 2  23.4 -2.87    2.27    7.32\n 3  21.6 -4.27  -11.9     4.69\n 4  24.0  0.137   0.263  11.7 \n 5  20.3 -3.85  -17.7   -51.1 \n 6  21.2 -2.07   -9.68   -2.43\n 7  24.3 -2.75   -0.193   6.20\n 8  26.2  1.21    0.923 -13.1 \n 9  22.6 -2.34   -3.93    7.80\n10  24.8  0.612   3.07   -1.46\n# ℹ 90 more rows\n\n## Plot x-y1 relationship\nggplot(data = polysim, mapping = aes(x = x, y = y1)) + geom_point()\n\n\n\n# ==============================================================================\n\n# LM for a linear relationship example\n\n## Fit linear model\nfit1 &lt;- lm(\n  formula = y1 ~ x,\n  data = polysim\n)\nmodel_parameters(fit1) |&gt; print_md()\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(98)\np\n\n\n\n\n(Intercept)\n-27.89\n0.92\n(-29.72, -26.06)\n-30.25\n&lt; .001\n\n\nx\n1.13\n0.04\n(1.06, 1.20)\n31.13\n&lt; .001\n\n\n\n\npredict_response(fit1, terms = \"x\") |&gt; plot(show_data = TRUE)\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1, check = \"linearity\") # good enough\n\n\n\n# ==============================================================================\n\n# LM with raw polynomials example\n\n## Fit raw polynomial model\nfit1b &lt;- lm(\n  formula = y1 ~ x + I(x^2),\n  data = polysim\n)\nmodel_parameters(fit1b) |&gt; print_md() # both are nonsignificant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n-25.97\n9.37\n(-44.57, -7.38)\n-2.77\n0.007\n\n\nx\n0.97\n0.75\n(-0.51, 2.46)\n1.30\n0.197\n\n\nx^2\n3.05e-03\n0.01\n(-0.03, 0.03)\n0.21\n0.838\n\n\n\n\npredict_response(fit1b, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity and collinearity assumptions\ncheck_model(fit1b, check = \"linearity\") # still acceptable\n\n\n\ncheck_collinearity(fit1b) # problematically high\n\n# Check for Multicollinearity\n\nHigh Correlation\n\n   Term    VIF       VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n      x 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n I(x^2) 423.33 [292.35, 613.19]        20.57  2.36e-03     [0.00, 0.00]\n\n# ==============================================================================\n\n# LM with orthogonal polynomials example\n\n## Fit orthogonal polynomial model\nfit1c &lt;- lm(\n  formula = y1 ~ poly(x, degree = 2),\n  data = polysim\n)\nmodel_parameters(fit1c) |&gt; print_md() # first degree is significant\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(97)\np\n\n\n\n\n(Intercept)\n0.65\n0.10\n(0.45, 0.85)\n6.54\n&lt; .001\n\n\nx (1st degree)\n30.78\n0.99\n(28.80, 32.75)\n30.98\n&lt; .001\n\n\nx (2nd degree)\n0.20\n0.99\n(-1.77, 2.18)\n0.21\n0.838\n\n\n\n\npredict_response(fit1c, terms = \"x\") |&gt; plot(show_data = TRUE) # unchanged\n\nData points may overlap. Use the `jitter` argument to add some amount of\n  random variation to the location of data points and avoid overplotting.\n\n\n\n\n## Check linearity assumption\ncheck_model(fit1c, check = \"linearity\") # still acceptable"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "4  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  }
]